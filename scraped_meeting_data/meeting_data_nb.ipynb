{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from loguru import logger\n",
    "import time\n",
    "# this is where any files or directories/path variables go:\n",
    "csv_filename = './meetings.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP A LOGURU LOGGER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = './data/meeting_scraper.log'\n",
    "rotation=\"100 MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(log_file_path, rotation=rotation, level=\"INFO\", format=\"[<green>{time: MMM D YYYY HH:mm:ss:SSSS}</>] | <level>{message}</>\", backtrace=True, diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A BASIC SCRAPER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CSV PARSER TO GET LINKS FROM CSV FILE:\n",
    "def csv_parser(csv_reader, header: str):\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_addresses(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        address_list = csv_parser(csv_reader, 'address')\n",
    "    return address_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_soup_data(link):\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL PARSE THE ROW DATA AND STORE IT \n",
    "# IN A DICTIONARY. THAT DICTIONARY CAN THEN BE INSERTED INTO \n",
    "# A LIST OF DICTIONARIES CALLED ROW_LIST BUT TO DO THIS THE PARSER\n",
    "# HAS TO JOIN LIST ITEMS INTO ONE LONG STRING SO EACH ROW HAS THE \n",
    "# SAME NUMBER OF COLUMNS:\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "\n",
    "# VERSION 1\n",
    "def row_parser1(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n",
    "\n",
    "# VERSION 2: \n",
    "def row_parser2(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "            # now add item1 to the row data\n",
    "            row['day'] = ' | '.join(item1['day'])\n",
    "            row['time'] = ' | '.join(item1['time'])\n",
    "            row['info'] = ' | '.join(item1['info'])\n",
    "            # now return the row data dictionary\n",
    "            return row\n",
    "        except Exception as e:\n",
    "            print(f'{e}')\n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "        for k, v in row.items():\n",
    "            if v is not None:\n",
    "                pass\n",
    "            else:\n",
    "                v = k\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE 'MAIN LOGICAL FUNCTION' THIS FUNCTION WILL COMBINE THE \n",
    "# get_address_data, get_table_data, and meeting_row_parser FUNCTIONS.\n",
    "# THAT WAY I CAN EXECUTE ALL OF THE FUNCTIONS IN ONE CALL.\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    # Get Soup Data\n",
    "    soup = fetch_soup_data(link)\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = row_parser1(d[0], d[1])\n",
    "\n",
    "    logger.info(\"[+] Main Meeting Dart Scraper Worked\")\n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return it.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: CREATE A CSV PARSER TO GET LINKS FROM CSV FILE:\n",
    "def csv_parser(csv_reader, header: str):\n",
    "    \"\"\"\n",
    "    :param csv_reader: This is a csv_reader object from the csv package\n",
    "    :param header: This is the header of the CSV file\n",
    "    :returns: a list of all the data from the CSV file row for row:\n",
    "    \"\"\"\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list\n",
    "\n",
    "# THIS FUNC USES THE csv_parser TO EXTRACT LINKS FROM THE meetings.csv file:\n",
    "def get_links(csv_filename):\n",
    "    \"\"\"\n",
    "    :param csv_filename: This is where the CSV file path that you want to read from goes\n",
    "    :returns: a list of links\n",
    "    \"\"\"\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_addresses(csv_filename):\n",
    "    \"\"\"\n",
    "    :param csv_filename: This is where the CSV file path that you want to read from goes\n",
    "    :returns: a list of addresses\n",
    "    \"\"\"\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        address_list = csv_parser(csv_reader, 'address')\n",
    "    return address_list\n",
    "\n",
    "\n",
    "def fetch_soup_data(link):\n",
    "    \"\"\"\n",
    "    Basic function that takes a link from link_list and accepts it as an argument, \n",
    "    then uses the requests.get method to return an html page. Then uses BeautifulSoup \n",
    "    to parse the pages' text using lxml\n",
    "    \"\"\"\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the address data from a soup object that gets past to it in a \n",
    "    for loop. \n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: A dictionary containing the address data components: {address, name, city, state}\n",
    "\n",
    "    ::: I need to figure out how to add zip_code to this :::\n",
    "    \"\"\"\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "\n",
    "\n",
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the meeting details data from a soup object that gets past to it in a \n",
    "    for loop. The data is extracted from an html table. Each row that returns a list is joined together to make \n",
    "    one string so it can be saved to a csv file.\n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: a dictionary with the days the meeting runs, the time, and info about the meeting: \n",
    "    {day, time, info}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        row['zip_code'] = ''\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv_filename = './meeting_details.csv'\n",
    "def csv_writer(row_data, csv_filename):\n",
    "    with open(csv_filename, 'w') as f:\n",
    "        writer = csv.DictWriter(f,  delimiter='|')\n",
    "        for row in row_data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC COMPONENTS BUILT..\n",
    "\n",
    "<hr>\n",
    "\n",
    "## CREATE LINK LISTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "3588\n"
     ]
    }
   ],
   "source": [
    "# whole list of links\n",
    "link_list = get_links(csv_filename)\n",
    "\n",
    "# shortened list: first 1000\n",
    "links_1000 = link_list[:1000]\n",
    "\n",
    "# shortened list: first 500\n",
    "links_500 = link_list[:500]\n",
    "\n",
    "links_100 = link_list[:100]\n",
    "\n",
    "####################################################################################\n",
    "# chunk the links into bunches of 10,000\n",
    "link_list1 = link_list[:10000]\n",
    "link_list2 = link_list[10000:20000]\n",
    "link_list3 = link_list[20000:30000]\n",
    "link_list4 = link_list[30000:]\n",
    "\n",
    "print(len(link_list1))\n",
    "print(len(link_list2))\n",
    "print(len(link_list3))\n",
    "print(len(link_list4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BUILD TWO FUNCTIONS PUTTING ALL THE ABOVE COMPONENTS TOGETHER\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_data_scraper(soup_data):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        rows = []\n",
    "        for soup in soup_data:\n",
    "            # Create two dicts with the following keys\n",
    "            address_dict = get_address_data(soup)\n",
    "            details_dict = get_table_data(soup)\n",
    "            logger.info(\"[+] Address and table data parsed\")\n",
    "            d = [address_dict, details_dict]\n",
    "            row_data = row_parser(d[0], d[1])\n",
    "            rows.append(row_data)\n",
    "        logger.info(\"++++++++ SCRAPED ALL THE LINKS +++++++++ \")\n",
    "        return rows\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: The list that is returned from the get_links function. Then it is passed to \n",
    "    scrape function that is used inside this function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup_data = scrape(link_list)\n",
    "        rows = soup_data_scraper(soup_data)\n",
    "        return rows\n",
    "\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        time.sleep(10)\n",
    "        logger.error(\"{} Request Exception occured\", re)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception while scraping link list\", e)\n",
    "\n",
    "new_csv_filename = './meeting_details.csv'\n",
    "\n",
    "def scrape(link_list, csv_file):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    row_data = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 500 links, write the row data to the new CSV file\n",
    "                for soup in soup_data:\n",
    "                    row = soup_data_scraper(soup)\n",
    "                    row_data.append(row)\n",
    "                # after each soup item is parsed and the results appended to row_data, append row data to csv\n",
    "                csv_writer(row_data, csv_file, headers)\n",
    "                # Then clear both soup and row data lists\n",
    "                soup_data.clear()\n",
    "                row_data.clear()\n",
    "                # log the success of the script\n",
    "                logger.info(\"[+] Count is divisible by 500:{} | Writing ROW_DATA to [{}] | soup_data and row_data cleared!\", count, csv_file)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 13:02:12.738 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-11 13:02:12.740 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:12.742 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 0\n",
      "2022-06-11 13:02:13.079 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-11 13:02:13.080 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:13.082 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 1\n",
      "2022-06-11 13:02:13.423 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-11 13:02:13.424 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:13.425 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 2\n",
      "2022-06-11 13:02:13.771 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-11 13:02:13.773 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:13.775 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 3\n",
      "2022-06-11 13:02:14.173 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-11 13:02:14.175 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:14.177 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 4\n",
      "2022-06-11 13:02:14.528 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-11 13:02:14.530 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:14.531 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 5\n",
      "2022-06-11 13:02:14.890 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-11 13:02:14.891 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:14.892 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 6\n",
      "2022-06-11 13:02:15.266 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-11 13:02:15.267 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:15.269 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 7\n",
      "2022-06-11 13:02:15.618 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-11 13:02:15.620 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:15.622 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 8\n",
      "2022-06-11 13:02:15.956 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-11 13:02:15.957 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:15.958 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 9\n",
      "2022-06-11 13:02:16.316 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-11 13:02:16.318 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:16.319 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 10\n",
      "2022-06-11 13:02:16.638 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-11 13:02:16.639 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:16.641 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 11\n",
      "2022-06-11 13:02:16.999 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-11 13:02:17.001 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:17.003 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 12\n",
      "2022-06-11 13:02:17.322 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-11 13:02:17.323 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:17.325 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 13\n",
      "2022-06-11 13:02:17.667 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-11 13:02:17.669 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:17.671 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 14\n",
      "2022-06-11 13:02:17.978 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-11 13:02:17.979 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:17.980 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 15\n",
      "2022-06-11 13:02:18.313 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-11 13:02:18.315 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:18.316 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 16\n",
      "2022-06-11 13:02:18.661 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-11 13:02:18.662 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:18.664 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 17\n",
      "2022-06-11 13:02:19.029 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-11 13:02:19.030 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:19.031 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 18\n",
      "2022-06-11 13:02:19.372 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-11 13:02:19.374 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:19.375 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 19\n",
      "2022-06-11 13:02:19.727 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-11 13:02:19.728 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:19.729 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 20\n",
      "2022-06-11 13:02:20.078 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-11 13:02:20.079 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:20.080 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 21\n",
      "2022-06-11 13:02:20.409 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-11 13:02:20.410 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:20.412 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 22\n",
      "2022-06-11 13:02:20.778 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-11 13:02:20.779 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:20.781 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 23\n",
      "2022-06-11 13:02:21.137 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/athens-group/\n",
      "2022-06-11 13:02:21.138 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:21.139 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/athens-group/ scraped successfully: Link number 24\n",
      "2022-06-11 13:02:21.457 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/\n",
      "2022-06-11 13:02:21.459 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:21.460 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/ scraped successfully: Link number 25\n",
      "2022-06-11 13:02:21.824 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/triangle-group-online/\n",
      "2022-06-11 13:02:21.825 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:21.826 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/triangle-group-online/ scraped successfully: Link number 26\n",
      "2022-06-11 13:02:22.211 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/\n",
      "2022-06-11 13:02:22.213 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:22.214 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/ scraped successfully: Link number 27\n",
      "2022-06-11 13:02:22.561 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/\n",
      "2022-06-11 13:02:22.563 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:22.564 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/ scraped successfully: Link number 28\n",
      "2022-06-11 13:02:22.911 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/discussion-anaheim/\n",
      "2022-06-11 13:02:22.912 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:22.913 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/discussion-anaheim/ scraped successfully: Link number 29\n",
      "2022-06-11 13:02:23.246 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/in-the-hall-group/\n",
      "2022-06-11 13:02:23.247 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:23.248 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/in-the-hall-group/ scraped successfully: Link number 30\n",
      "2022-06-11 13:02:23.587 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/shining-light-group/\n",
      "2022-06-11 13:02:23.589 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:23.590 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/shining-light-group/ scraped successfully: Link number 31\n",
      "2022-06-11 13:02:23.942 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/\n",
      "2022-06-11 13:02:23.943 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:23.945 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/ scraped successfully: Link number 32\n",
      "2022-06-11 13:02:24.268 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chester-big-book-group/\n",
      "2022-06-11 13:02:24.269 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:24.271 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/chester-big-book-group/ scraped successfully: Link number 33\n",
      "2022-06-11 13:02:24.662 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/\n",
      "2022-06-11 13:02:24.664 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:24.665 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/ scraped successfully: Link number 34\n",
      "2022-06-11 13:02:25.013 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/\n",
      "2022-06-11 13:02:25.015 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:25.016 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/ scraped successfully: Link number 35\n",
      "2022-06-11 13:02:25.367 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/\n",
      "2022-06-11 13:02:25.368 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:25.370 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/ scraped successfully: Link number 36\n",
      "2022-06-11 13:02:25.746 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/pali-womens-group/\n",
      "2022-06-11 13:02:25.748 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:25.750 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/pali-womens-group/ scraped successfully: Link number 37\n",
      "2022-06-11 13:02:26.065 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/ohua-group/\n",
      "2022-06-11 13:02:26.066 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:26.067 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/ohua-group/ scraped successfully: Link number 38\n",
      "2022-06-11 13:02:26.369 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/\n",
      "2022-06-11 13:02:26.370 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:26.371 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/ scraped successfully: Link number 39\n",
      "2022-06-11 13:02:26.685 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/malia-discussion/\n",
      "2022-06-11 13:02:26.687 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:26.688 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/malia-discussion/ scraped successfully: Link number 40\n",
      "2022-06-11 13:02:27.346 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/\n",
      "2022-06-11 13:02:27.348 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:27.350 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/ scraped successfully: Link number 41\n",
      "2022-06-11 13:02:27.695 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/\n",
      "2022-06-11 13:02:27.696 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:27.697 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/ scraped successfully: Link number 42\n",
      "2022-06-11 13:02:28.000 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-23-anoka/\n",
      "2022-06-11 13:02:28.002 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:28.004 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/squad-23-anoka/ scraped successfully: Link number 43\n",
      "2022-06-11 13:02:28.324 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-20-anoka/\n",
      "2022-06-11 13:02:28.326 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:28.327 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/squad-20-anoka/ scraped successfully: Link number 44\n",
      "2022-06-11 13:02:28.631 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-fever/\n",
      "2022-06-11 13:02:28.632 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:28.633 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-fever/ scraped successfully: Link number 45\n",
      "2022-06-11 13:02:28.962 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-2-anoka/\n",
      "2022-06-11 13:02:28.963 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:28.965 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/squad-2-anoka/ scraped successfully: Link number 46\n",
      "2022-06-11 13:02:29.278 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-18-anoka/\n",
      "2022-06-11 13:02:29.279 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:29.281 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/squad-18-anoka/ scraped successfully: Link number 47\n",
      "2022-06-11 13:02:29.617 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/\n",
      "2022-06-11 13:02:29.619 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:29.621 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/ scraped successfully: Link number 48\n",
      "2022-06-11 13:02:29.934 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/\n",
      "2022-06-11 13:02:29.935 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:29.936 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/ scraped successfully: Link number 49\n",
      "2022-06-11 13:02:30.266 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/wendover-group/\n",
      "2022-06-11 13:02:30.267 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:30.269 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/wendover-group/ scraped successfully: Link number 50\n",
      "2022-06-11 13:02:30.654 | INFO     | __main__:fetch_soup_data:51 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-meeting-sparks/\n",
      "2022-06-11 13:02:30.656 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 13:02:30.657 | INFO     | __main__:scrape:36 - [+] https://www.aa-meetings.com/aa-meeting/big-book-meeting-sparks/ scraped successfully: Link number 51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000023?line=0'>1</a>\u001b[0m scrape(links_500, new_csv_filename)\n",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 24'\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(link_list, csv_file)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=30'>31</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m link_list:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=32'>33</a>\u001b[0m     soup \u001b[39m=\u001b[39m fetch_soup_data(link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=33'>34</a>\u001b[0m     row \u001b[39m=\u001b[39m soup_data_scraper(soup)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=34'>35</a>\u001b[0m     row_data\u001b[39m.\u001b[39mappend(row)\n",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 15'\u001b[0m in \u001b[0;36mfetch_soup_data\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=41'>42</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=42'>43</a>\u001b[0m \u001b[39mBasic function that takes a link from link_list and accepts it as an argument, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=43'>44</a>\u001b[0m \u001b[39mthen uses the requests.get method to return an html page. Then uses BeautifulSoup \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=44'>45</a>\u001b[0m \u001b[39mto parse the pages' text using lxml\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=45'>46</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=46'>47</a>\u001b[0m \u001b[39m# STEP 1: Get the webpage response obj from requests\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=47'>48</a>\u001b[0m page \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=48'>49</a>\u001b[0m \u001b[39m# STEP 2: Get Soup object for html parsing\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=49'>50</a>\u001b[0m soup \u001b[39m=\u001b[39m bs(page\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=64'>65</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=65'>66</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=66'>67</a>\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=71'>72</a>\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=72'>73</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=56'>57</a>\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=57'>58</a>\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=58'>59</a>\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=523'>524</a>\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=524'>525</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=525'>526</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=526'>527</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=527'>528</a>\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=528'>529</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=530'>531</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=641'>642</a>\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=643'>644</a>\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=644'>645</a>\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=646'>647</a>\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=647'>648</a>\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=437'>438</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=439'>440</a>\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=440'>441</a>\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=441'>442</a>\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=442'>443</a>\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=443'>444</a>\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=444'>445</a>\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=445'>446</a>\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=446'>447</a>\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=447'>448</a>\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=448'>449</a>\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=449'>450</a>\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=450'>451</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=452'>453</a>\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=453'>454</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=454'>455</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=699'>700</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=701'>702</a>\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=702'>703</a>\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=703'>704</a>\u001b[0m     conn,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=704'>705</a>\u001b[0m     method,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=705'>706</a>\u001b[0m     url,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=706'>707</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=707'>708</a>\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=708'>709</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=709'>710</a>\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=710'>711</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=712'>713</a>\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=713'>714</a>\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=714'>715</a>\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=715'>716</a>\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=716'>717</a>\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=443'>444</a>\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=444'>445</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=445'>446</a>\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=446'>447</a>\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=447'>448</a>\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=448'>449</a>\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=449'>450</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=450'>451</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=440'>441</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=441'>442</a>\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=442'>443</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=443'>444</a>\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=444'>445</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=445'>446</a>\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=446'>447</a>\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=447'>448</a>\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=448'>449</a>\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:1371\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/http/client.py?line=1368'>1369</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/http/client.py?line=1369'>1370</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/lib/python3.9/http/client.py?line=1370'>1371</a>\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/http/client.py?line=1371'>1372</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/http/client.py?line=1372'>1373</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:319\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=316'>317</a>\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=317'>318</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.9/http/client.py?line=318'>319</a>\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=319'>320</a>\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=320'>321</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/http/client.py:280\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=278'>279</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///usr/lib/python3.9/http/client.py?line=279'>280</a>\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=280'>281</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/http/client.py?line=281'>282</a>\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/socket.py?line=701'>702</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/socket.py?line=702'>703</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/lib/python3.9/socket.py?line=703'>704</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/socket.py?line=704'>705</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/socket.py?line=705'>706</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1236'>1237</a>\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1237'>1238</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1238'>1239</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1239'>1240</a>\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> <a href='file:///usr/lib/python3.9/ssl.py?line=1240'>1241</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1241'>1242</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1242'>1243</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1096'>1097</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1097'>1098</a>\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///usr/lib/python3.9/ssl.py?line=1098'>1099</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1099'>1100</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1100'>1101</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape(links_500, new_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped to 13180\n",
    "csv_filename = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "def csv_writer(row_data, csv_filename, fields):\n",
    "    with open(csv_filename, 'w') as f:\n",
    "        header = csv.writer(f, delimiter='|')\n",
    "        header.writerow(fields)\n",
    "        writer = csv.DictWriter(f, fieldnames=fields, delimiter='|')\n",
    "        for row in row_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "csv_writer(row_data1, csv_filename, headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000036?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(row_data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'row_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc6d6da70282c31f5da581b322c5ad8b3a84bc6a483a690ad4c5cde2871c01ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
