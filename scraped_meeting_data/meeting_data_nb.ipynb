{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from loguru import logger\n",
    "import time\n",
    "# this is where any files or directories/path variables go:\n",
    "csv_filename = './meetings.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP A LOGURU LOGGER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = './data/meeting_scraper.log'\n",
    "rotation=\"100 MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(log_file_path, rotation=\"100 MB\", colorize=True, format=\"[<green>{time:MMM D, YYYY - HH:MM:ss}</green>] | <level>{message}</level>\", backtrace=True, diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A BASIC SCRAPER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CSV PARSER TO GET LINKS FROM CSV FILE:\n",
    "def csv_parser(csv_reader, header: str):\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_addresses(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        address_list = csv_parser(csv_reader, 'address')\n",
    "    return address_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_soup_data(link):\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL PARSE THE ROW DATA AND STORE IT \n",
    "# IN A DICTIONARY. THAT DICTIONARY CAN THEN BE INSERTED INTO \n",
    "# A LIST OF DICTIONARIES CALLED ROW_LIST BUT TO DO THIS THE PARSER\n",
    "# HAS TO JOIN LIST ITEMS INTO ONE LONG STRING SO EACH ROW HAS THE \n",
    "# SAME NUMBER OF COLUMNS:\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "\n",
    "# VERSION 1\n",
    "def row_parser1(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n",
    "\n",
    "# VERSION 2: \n",
    "def row_parser2(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "            # now add item1 to the row data\n",
    "            row['day'] = ' | '.join(item1['day'])\n",
    "            row['time'] = ' | '.join(item1['time'])\n",
    "            row['info'] = ' | '.join(item1['info'])\n",
    "            # now return the row data dictionary\n",
    "            return row\n",
    "        except Exception as e:\n",
    "            print(f'{e}')\n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "        for k, v in row.items():\n",
    "            if v is not None:\n",
    "                pass\n",
    "            else:\n",
    "                v = k\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE 'MAIN LOGICAL FUNCTION' THIS FUNCTION WILL COMBINE THE \n",
    "# get_address_data, get_table_data, and meeting_row_parser FUNCTIONS.\n",
    "# THAT WAY I CAN EXECUTE ALL OF THE FUNCTIONS IN ONE CALL.\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    # Get Soup Data\n",
    "    soup = fetch_soup_data(link)\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = row_parser1(d[0], d[1])\n",
    "\n",
    "    logger.info(\"[+] Main Meeting Dart Scraper Worked\")\n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return it.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: CREATE A CSV PARSER TO GET LINKS FROM CSV FILE:\n",
    "def csv_parser(csv_reader, header: str):\n",
    "    \"\"\"\n",
    "    :param csv_reader: This is a csv_reader object from the csv package\n",
    "    :param header: This is the header of the CSV file\n",
    "    :returns: a list of all the data from the CSV file row for row:\n",
    "    \"\"\"\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list\n",
    "\n",
    "# THIS FUNC USES THE csv_parser TO EXTRACT LINKS FROM THE meetings.csv file:\n",
    "def get_links(csv_filename):\n",
    "    \"\"\"\n",
    "    :param csv_filename: This is where the CSV file path that you want to read from goes\n",
    "    :returns: a list of links\n",
    "    \"\"\"\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_addresses(csv_filename):\n",
    "    \"\"\"\n",
    "    :param csv_filename: This is where the CSV file path that you want to read from goes\n",
    "    :returns: a list of addresses\n",
    "    \"\"\"\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        address_list = csv_parser(csv_reader, 'address')\n",
    "    return address_list\n",
    "\n",
    "\n",
    "def fetch_soup_data(link):\n",
    "    \"\"\"\n",
    "    Basic function that takes a link from link_list and accepts it as an argument, \n",
    "    then uses the requests.get method to return an html page. Then uses BeautifulSoup \n",
    "    to parse the pages' text using lxml\n",
    "    \"\"\"\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the address data from a soup object that gets past to it in a \n",
    "    for loop. \n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: A dictionary containing the address data components: {address, name, city, state}\n",
    "\n",
    "    ::: I need to figure out how to add zip_code to this :::\n",
    "    \"\"\"\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "\n",
    "\n",
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the meeting details data from a soup object that gets past to it in a \n",
    "    for loop. The data is extracted from an html table. Each row that returns a list is joined together to make \n",
    "    one string so it can be saved to a csv file.\n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: a dictionary with the days the meeting runs, the time, and info about the meeting: \n",
    "    {day, time, info}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC COMPONENTS BUILT..\n",
    "\n",
    "<hr>\n",
    "\n",
    "## CREATE LINK LISTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole list of links\n",
    "link_list = get_links(csv_filename)\n",
    "\n",
    "# shortened list: first 1000\n",
    "links_1000 = link_list[:1000]\n",
    "\n",
    "# shortened list: first 500\n",
    "links_500 = link_list[:500]\n",
    "\n",
    "links_100 = link_list[:100]\n",
    "\n",
    "####################################################################################\n",
    "# chunk the links into bunches of 10,000\n",
    "link_list1 = link_list[:10000]\n",
    "link_list2 = link_list[10000:20000]\n",
    "link_list3 = link_list[20000:30000]\n",
    "link_list4 = link_list[30000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BUILD TWO FUNCTIONS PUTTING ALL THE ABOVE COMPONENTS TOGETHER\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    soup_data = []\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            link_list.pop()\n",
    "        return soup_data\n",
    "    \n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_data_scraper(soup_data):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        rows = []\n",
    "        for soup in soup_data:\n",
    "            # Create two dicts with the following keys\n",
    "            address_dict = get_address_data(soup)\n",
    "            details_dict = get_table_data(soup)\n",
    "            logger.info(\"[+] Address and table data parsed\")\n",
    "            d = [address_dict, details_dict]\n",
    "            row_data = row_parser(d[0], d[1])\n",
    "            rows.append(row_data)\n",
    "        logger.info(\"[+] -----> SUCCESS <------\")\n",
    "        return rows\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: The list that is returned from the get_links function. Then it is passed to \n",
    "    scrape function that is used inside this function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        soup_data = scrape(link_list)\n",
    "        rows = soup_data_scraper(soup_data)\n",
    "        return rows\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception while scraping link list\", e)\n",
    "\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        time.sleep(10)\n",
    "        logger.error(\"{} Request Exception occured\", re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000023?line=0'>1</a>\u001b[0m row_data \u001b[39m=\u001b[39m get_rows(link_list1)\n",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 23'\u001b[0m in \u001b[0;36mget_rows\u001b[0;34m(link_list)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=2'>3</a>\u001b[0m \u001b[39m:param link_list: The list that is returned from the get_links function. Then it is passed to \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=3'>4</a>\u001b[0m \u001b[39mscrape function that is used inside this function. \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=6'>7</a>\u001b[0m \u001b[39mcreates a row of data for the csv file. \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=7'>8</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=8'>9</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=10'>11</a>\u001b[0m     soup_data \u001b[39m=\u001b[39m scrape(link_list)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=11'>12</a>\u001b[0m     rows \u001b[39m=\u001b[39m soup_data_scraper(soup_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000022?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rows\n",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 21'\u001b[0m in \u001b[0;36mscrape\u001b[0;34m(link_list)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000020?line=7'>8</a>\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000020?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m link_list:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000020?line=9'>10</a>\u001b[0m     soup \u001b[39m=\u001b[39m fetch_soup_data(link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000020?line=10'>11</a>\u001b[0m     soup_data\u001b[39m.\u001b[39mappend(soup)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000020?line=11'>12</a>\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m[+] \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m scraped successfully: Link number \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m, link, count)\n",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 15'\u001b[0m in \u001b[0;36mfetch_soup_data\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=41'>42</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=42'>43</a>\u001b[0m \u001b[39mBasic function that takes a link from link_list and accepts it as an argument, \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=43'>44</a>\u001b[0m \u001b[39mthen uses the requests.get method to return an html page. Then uses BeautifulSoup \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=44'>45</a>\u001b[0m \u001b[39mto parse the pages' text using lxml\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=45'>46</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=46'>47</a>\u001b[0m \u001b[39m# STEP 1: Get the webpage response obj from requests\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=47'>48</a>\u001b[0m page \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(link)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=48'>49</a>\u001b[0m \u001b[39m# STEP 2: Get Soup object for html parsing\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000014?line=49'>50</a>\u001b[0m soup \u001b[39m=\u001b[39m bs(page\u001b[39m.\u001b[39mtext, \u001b[39m\"\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py:75\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=63'>64</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=64'>65</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=65'>66</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=66'>67</a>\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=71'>72</a>\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=72'>73</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=74'>75</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=56'>57</a>\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=57'>58</a>\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=58'>59</a>\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/api.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=523'>524</a>\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=524'>525</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=525'>526</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=526'>527</a>\u001b[0m }\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=527'>528</a>\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=528'>529</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=530'>531</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=641'>642</a>\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=643'>644</a>\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=644'>645</a>\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=646'>647</a>\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/sessions.py?line=647'>648</a>\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=437'>438</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=438'>439</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=439'>440</a>\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=440'>441</a>\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=441'>442</a>\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=442'>443</a>\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=443'>444</a>\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=444'>445</a>\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=445'>446</a>\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=446'>447</a>\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=447'>448</a>\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=448'>449</a>\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=449'>450</a>\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=450'>451</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=452'>453</a>\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=453'>454</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/requests/adapters.py?line=454'>455</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=699'>700</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=701'>702</a>\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=702'>703</a>\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=703'>704</a>\u001b[0m     conn,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=704'>705</a>\u001b[0m     method,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=705'>706</a>\u001b[0m     url,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=706'>707</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=707'>708</a>\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=708'>709</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=709'>710</a>\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=710'>711</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=712'>713</a>\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=713'>714</a>\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=714'>715</a>\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=715'>716</a>\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=716'>717</a>\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=383'>384</a>\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=384'>385</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=385'>386</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=386'>387</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=387'>388</a>\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=388'>389</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py:1040\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1037'>1038</a>\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1038'>1039</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1039'>1040</a>\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1041'>1042</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1042'>1043</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1043'>1044</a>\u001b[0m         (\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1044'>1045</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1049'>1050</a>\u001b[0m         InsecureRequestWarning,\n\u001b[1;32m   <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connectionpool.py?line=1050'>1051</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=404'>405</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=405'>406</a>\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=406'>407</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=409'>410</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=410'>411</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=411'>412</a>\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=413'>414</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=414'>415</a>\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=415'>416</a>\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=416'>417</a>\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=417'>418</a>\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=418'>419</a>\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=419'>420</a>\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=420'>421</a>\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=421'>422</a>\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=422'>423</a>\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=423'>424</a>\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=424'>425</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=426'>427</a>\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=427'>428</a>\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=428'>429</a>\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=429'>430</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=430'>431</a>\u001b[0m     default_ssl_context\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=431'>432</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=432'>433</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=433'>434</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/connection.py?line=434'>435</a>\u001b[0m ):\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=436'>437</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=437'>438</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=438'>439</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=444'>445</a>\u001b[0m         SNIMissingWarning,\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=445'>446</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=447'>448</a>\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=448'>449</a>\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=449'>450</a>\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=450'>451</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=451'>452</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=452'>453</a>\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[0;32m~/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=489'>490</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=491'>492</a>\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=492'>493</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=493'>494</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/blackhood/.virtualenvs/scraper/lib/python3.9/site-packages/urllib3/util/ssl_.py?line=494'>495</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:500\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=493'>494</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=494'>495</a>\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=495'>496</a>\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=496'>497</a>\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=497'>498</a>\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=498'>499</a>\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/lib/python3.9/ssl.py?line=499'>500</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=500'>501</a>\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=501'>502</a>\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=502'>503</a>\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=503'>504</a>\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=504'>505</a>\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=505'>506</a>\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=506'>507</a>\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    <a href='file:///usr/lib/python3.9/ssl.py?line=507'>508</a>\u001b[0m     )\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1040\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1036'>1037</a>\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1037'>1038</a>\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1038'>1039</a>\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> <a href='file:///usr/lib/python3.9/ssl.py?line=1039'>1040</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1040'>1041</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1041'>1042</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.9/ssl.py:1309\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1306'>1307</a>\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1307'>1308</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> <a href='file:///usr/lib/python3.9/ssl.py?line=1308'>1309</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1309'>1310</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   <a href='file:///usr/lib/python3.9/ssl.py?line=1310'>1311</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "row_data = get_rows(link_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped to 13180\n",
    "csv_filename = './meeting_details.csv'\n",
    "def csv_writer(row_data, csv_filename):\n",
    "    with open(csv_filename, 'w') as f:   \n",
    "        writer = csv.writer(f)\n",
    "        for row in row_data:\n",
    "            for k,v in row.items():\n",
    "                writer.writerow(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000036?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(row_data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'row_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc6d6da70282c31f5da581b322c5ad8b3a84bc6a483a690ad4c5cde2871c01ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('scraper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
