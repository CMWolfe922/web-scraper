{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from loguru import logger\n",
    "import time\n",
    "# this is where any files or directories/path variables go:\n",
    "csv_filename = './meetings.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP A LOGURU LOGGER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = './data/meeting_scraper.log'\n",
    "rotation=\"100 MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(log_file_path, rotation=rotation, level=\"INFO\", format=\"[<green>{time: MMM D YYYY HH:mm:ss:SSSS}</>] | <level>{message}</>\", backtrace=True, diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A BASIC SCRAPER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_list_from_column_data(csv_filename, column):\n",
    "    \"\"\"\n",
    "    :description: Create a list from a CSV column's data. To do this I will use the with open()\n",
    "    context manager and open the csv file that was passed to this function as an argument.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :param csv_filename:-> This is the name of the CSV file containing the column data I want to extract\n",
    "    to create a list.\n",
    "    :param column:-> This is the column name or number to extract data from in the CSV file.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :return:-> A list of strings/integers/float/datetypes from a CSV file.\n",
    "    \"\"\"\n",
    "    # now create an empty list to append data to:\n",
    "    data_list = []\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        _column = next(csv_reader)\n",
    "        columnIndex = _column.index(column)\n",
    "        # loop through CSV file and append to address_list\n",
    "        for line in csv_reader:\n",
    "            all_data = line[columnIndex]\n",
    "            data_list.append(all_data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_soup_data(link):\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"Retrieved soup_data for link: {}\", link)\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL PARSE THE ROW DATA AND STORE IT \n",
    "# IN A DICTIONARY. THAT DICTIONARY CAN THEN BE INSERTED INTO \n",
    "# A LIST OF DICTIONARIES CALLED ROW_LIST BUT TO DO THIS THE PARSER\n",
    "# HAS TO JOIN LIST ITEMS INTO ONE LONG STRING SO EACH ROW HAS THE \n",
    "# SAME NUMBER OF COLUMNS:\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "\n",
    "# VERSION 1\n",
    "def parse_dicts(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {'name':[], 'address':[], 'city':[], 'state':[], 'zip_code':[], 'day':[], 'time':[], 'info':[]}\n",
    "    try:\n",
    "\n",
    "        try:\n",
    "            row['name'].append(item0['name'])\n",
    "            row['address'].append(item0['address'])\n",
    "            row['city'].append(item0['city'])\n",
    "            row['state'].append(item0['state'])\n",
    "            row['zip_code'].append('00000')\n",
    "        except Exception as e:\n",
    "            logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "            print(e)\n",
    "            row['name'].append('name')\n",
    "            row['address'].append('address')\n",
    "            row['city'].append('city')\n",
    "            row['state'].append('state')\n",
    "            row['zip_code'].append('zip_code')\n",
    "\n",
    "        # now add item1 to the row data\n",
    "        row['day'].append(' | '.join(item1['day']))\n",
    "        row['time'].append(' | '.join(item1['time']))\n",
    "        row['info'].append(' | '.join(item1['info']))\n",
    "\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "        return row\n",
    "    except Exception as e:\n",
    "        logger.error(\"Exception Raised: {}\", e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE 'MAIN LOGICAL FUNCTION' THIS FUNCTION WILL COMBINE THE \n",
    "# get_address_data, get_table_data, and meeting_row_parser FUNCTIONS.\n",
    "# THAT WAY I CAN EXECUTE ALL OF THE FUNCTIONS IN ONE CALL.\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    # Get Soup Data\n",
    "    soup = fetch_soup_data(link)\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = row_parser1(d[0], d[1])\n",
    "\n",
    "    logger.info(\"[+] Main Meeting Dart Scraper Worked\")\n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return it.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_column_data(csv_filename, column):\n",
    "    \"\"\"\n",
    "    :description: Create a list from a CSV column's data. To do this I will use the with open()\n",
    "    context manager and open the csv file that was passed to this function as an argument.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :param csv_filename:-> This is the name of the CSV file containing the column data I want to extract\n",
    "    to create a list.\n",
    "    :param column:-> This is the column name or number to extract data from in the CSV file.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :return:-> A list of strings/integers/float/datetypes from a CSV file.\n",
    "    \"\"\"\n",
    "    # now create an empty list to append data to:\n",
    "    data_list = []\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        _column = next(csv_reader)\n",
    "        columnIndex = _column.index(column)\n",
    "        # loop through CSV file and append to address_list\n",
    "        for line in csv_reader:\n",
    "            all_data = line[columnIndex]\n",
    "            data_list.append(all_data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def fetch_soup_data(link):\n",
    "    \"\"\"\n",
    "    Basic function that takes a link from link_list and accepts it as an argument, \n",
    "    then uses the requests.get method to return an html page. Then uses BeautifulSoup \n",
    "    to parse the pages' text using lxml\n",
    "    \"\"\"\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the address data from a soup object that gets past to it in a \n",
    "    for loop. \n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: A dictionary containing the address data components: {address, name, city, state}\n",
    "\n",
    "    ::: I need to figure out how to add zip_code to this :::\n",
    "    \"\"\"\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "\n",
    "\n",
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the meeting details data from a soup object that gets past to it in a \n",
    "    for loop. The data is extracted from an html table. Each row that returns a list is joined together to make \n",
    "    one string so it can be saved to a csv file.\n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: a dictionary with the days the meeting runs, the time, and info about the meeting: \n",
    "    {day, time, info}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        row['zip_code'] = '00000'\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = '|'.join(item1['day'].replace(',', '-'))\n",
    "    row['time'] = '|'.join(item1['time'].replace(',', '-'))\n",
    "    row['info'] = '|'.join(item1['info'].replace(',', '-'))\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv_file = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "def csv_writer(row_data, csv_filename, headers=None):\n",
    "    with open(csv_filename, 'a') as f:\n",
    "        if headers is not None:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            # writer.writeheader(headers)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            # fieldnames = [k for k in row_data.keys()]\n",
    "            # writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='|')\n",
    "            # writer.writeheader(fieldnames)\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC COMPONENTS BUILT..\n",
    "\n",
    "<hr>\n",
    "\n",
    "## CREATE LINK LISTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole list of links\n",
    "link_list = create_list_from_column_data(csv_filename, 'link')\n",
    "\n",
    "# shortened list: first 1000\n",
    "links_1000 = link_list[:1000]\n",
    "\n",
    "# shortened list: first 500\n",
    "links_500 = link_list[:500]\n",
    "\n",
    "links_100 = link_list[:100]\n",
    "\n",
    "####################################################################################\n",
    "# chunk the links into bunches of 10,000\n",
    "link_list1 = link_list[:10000]\n",
    "link_list2 = link_list[10000:20000]\n",
    "link_list3 = link_list[20000:30000]\n",
    "link_list4 = link_list[30000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BUILD TWO FUNCTIONS PUTTING ALL THE ABOVE COMPONENTS TOGETHER\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_list_scraper(soup_data):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        rows = {'name':[], 'address':[], 'city':[], 'state':[], 'zip_code':[], 'day':[], 'time':[], 'info':[]}\n",
    "        for soup in soup_data:\n",
    "            # Create two dicts with the following keys\n",
    "            address_dict = get_address_data(soup)\n",
    "            details_dict = get_table_data(soup)\n",
    "            logger.info(\"[+] Address and table data parsed\")\n",
    "            d = [address_dict, details_dict]\n",
    "            row_data = parse_dicts(d[0], d[1])\n",
    "            rows['name'].append(row_data['name'])\n",
    "            rows['address'].append(row_data['address'])\n",
    "            rows['city'].append(row_data['city'])\n",
    "            rows['state'].append(row_data['state'])\n",
    "            rows['zip_code'].append(row_data['zip_code'])\n",
    "            rows['day'].append(row_data['day'])\n",
    "            rows['time'].append(row_data['time'])\n",
    "            rows['info'].append(row_data['info'])\n",
    "\n",
    "        logger.info(\"++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \")\n",
    "        return rows\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)\n",
    "        \n",
    "\n",
    "def single_soup_scraper(soup):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create two dicts with the following keys\n",
    "        address_dict = get_address_data(soup)\n",
    "        details_dict = get_table_data(soup)\n",
    "        d = [address_dict, details_dict]\n",
    "        row = row_parser(d[0], d[1])\n",
    "        logger.info(\n",
    "            \"[+] Parsed Address and Table Data | Row [{}] Created\", row)\n",
    "        return row\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: The list that is returned from the get_links function. Then it is passed to \n",
    "    scrape function that is used inside this function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup_data = scrape(link_list)\n",
    "        rows = soup_list_scraper(soup_data)\n",
    "        return rows\n",
    "\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        time.sleep(10)\n",
    "        logger.error(\"{} Request Exception occured\", re)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception while scraping link list\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link_list, csv_file):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    row_data = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state',\n",
    "               'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\n",
    "                \"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 500 links, write the row data to the new CSV file\n",
    "                for soup in soup_data:\n",
    "                    row = single_soup_scraper(soup)\n",
    "                    row_data.append(row)\n",
    "                # check that row_data has 500 items:\n",
    "                    if len(row_data) == 500:\n",
    "                        # after each soup item is parsed and the results appended to row_data, append row data to csv\n",
    "                        csv_writer(row_data, csv_file, headers)\n",
    "                        # Then clear both soup and row data lists\n",
    "                        soup_data.clear()\n",
    "                        row_data.clear()\n",
    "                        # log the success of the script\n",
    "                        logger.info(\n",
    "                            \"[+] Count is divisible by 500:{} | Writing ROW_DATA to [{}] | soup_data and row_data cleared!\", count, csv_file)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv_filename = './meeting_details.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    dataframes = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state',\n",
    "               'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\n",
    "                \"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 1000 links, write the row data to the new CSV file\n",
    "                rows = soup_list_scraper(soup_data)\n",
    "                row_df = pd.DataFrame.from_dict(rows)\n",
    "                row_df.to_csv(new_csv_file, mode='a', sep=\"\\t\", index=False)\n",
    "                soup_data.clear()\n",
    "                logger.info(\n",
    "                            \"[+] Count is divisible by 1000:{} | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\", count)\n",
    "\n",
    "        print(dataframes)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 07:46:31.982 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-15 07:46:31.983 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 0\n",
      "2022-06-15 07:46:32.246 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-15 07:46:32.248 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 1\n",
      "2022-06-15 07:46:32.497 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-15 07:46:32.498 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 2\n",
      "2022-06-15 07:46:32.820 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-15 07:46:32.821 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 3\n",
      "2022-06-15 07:46:33.080 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-15 07:46:33.081 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 4\n",
      "2022-06-15 07:46:33.341 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-15 07:46:33.343 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 5\n",
      "2022-06-15 07:46:33.611 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-15 07:46:33.612 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 6\n",
      "2022-06-15 07:46:33.871 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-15 07:46:33.872 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 7\n",
      "2022-06-15 07:46:34.133 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-15 07:46:34.135 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 8\n",
      "2022-06-15 07:46:34.381 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-15 07:46:34.382 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 9\n",
      "2022-06-15 07:46:39.390 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.404 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.408 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.411 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.418 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.431 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.433 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.436 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.440 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.447 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.448 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.450 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.452 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.456 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.457 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.458 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.460 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.464 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.466 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.466 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.468 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.471 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.472 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.473 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.474 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.480 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.481 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.482 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.484 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.488 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.489 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.490 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.491 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.494 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.495 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.496 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.497 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:39.501 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:39.502 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:39.503 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:39.504 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:46:39.506 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:10 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:46:39.777 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-15 07:46:39.778 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 10\n",
      "2022-06-15 07:46:40.053 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-15 07:46:40.055 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 11\n",
      "2022-06-15 07:46:40.327 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-15 07:46:40.329 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 12\n",
      "2022-06-15 07:46:40.601 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-15 07:46:40.602 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 13\n",
      "2022-06-15 07:46:40.853 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-15 07:46:40.854 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 14\n",
      "2022-06-15 07:46:41.127 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-15 07:46:41.197 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 15\n",
      "2022-06-15 07:46:41.469 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-15 07:46:41.471 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 16\n",
      "2022-06-15 07:46:41.715 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-15 07:46:41.717 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 17\n",
      "2022-06-15 07:46:41.989 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-15 07:46:41.991 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 18\n",
      "2022-06-15 07:46:42.261 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-15 07:46:42.262 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 19\n",
      "2022-06-15 07:46:47.269 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.293 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.295 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.297 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.299 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.307 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.308 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.309 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.312 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.316 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.317 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.318 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.320 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.323 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.324 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.325 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.326 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.329 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.330 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.330 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.332 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.335 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.337 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.338 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.340 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.346 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.347 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.347 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.352 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.356 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.357 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.358 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.360 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.366 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.367 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.368 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.369 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:47.373 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:47.374 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:47.375 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:47.376 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:46:47.379 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:20 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:46:47.623 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-15 07:46:47.624 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 20\n",
      "2022-06-15 07:46:47.878 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-15 07:46:47.879 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 21\n",
      "2022-06-15 07:46:48.136 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-15 07:46:48.138 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 22\n",
      "2022-06-15 07:46:48.410 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-15 07:46:48.411 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 23\n",
      "2022-06-15 07:46:48.655 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/athens-group/\n",
      "2022-06-15 07:46:48.657 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/athens-group/ scraped successfully: Link number 24\n",
      "2022-06-15 07:46:48.918 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/\n",
      "2022-06-15 07:46:48.920 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/ scraped successfully: Link number 25\n",
      "2022-06-15 07:46:49.187 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/triangle-group-online/\n",
      "2022-06-15 07:46:49.188 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/triangle-group-online/ scraped successfully: Link number 26\n",
      "2022-06-15 07:46:49.454 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/\n",
      "2022-06-15 07:46:49.456 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/ scraped successfully: Link number 27\n",
      "2022-06-15 07:46:49.730 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/\n",
      "2022-06-15 07:46:49.732 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/ scraped successfully: Link number 28\n",
      "2022-06-15 07:46:50.004 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/discussion-anaheim/\n",
      "2022-06-15 07:46:50.005 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/discussion-anaheim/ scraped successfully: Link number 29\n",
      "2022-06-15 07:46:55.013 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.026 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.030 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.033 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.038 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.048 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.051 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.052 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.055 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.070 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.072 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.073 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.074 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.080 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.081 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.082 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.083 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.091 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.092 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.093 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.094 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.097 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.098 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.099 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.100 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.104 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.105 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.106 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.107 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.110 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.111 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.112 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.113 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.125 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.126 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.127 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.128 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:46:55.135 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:46:55.136 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:46:55.137 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:46:55.138 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:46:55.140 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:30 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:46:55.400 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/in-the-hall-group/\n",
      "2022-06-15 07:46:55.401 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/in-the-hall-group/ scraped successfully: Link number 30\n",
      "2022-06-15 07:46:55.674 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/shining-light-group/\n",
      "2022-06-15 07:46:55.675 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/shining-light-group/ scraped successfully: Link number 31\n",
      "2022-06-15 07:46:55.940 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/\n",
      "2022-06-15 07:46:55.941 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/ scraped successfully: Link number 32\n",
      "2022-06-15 07:46:56.189 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chester-big-book-group/\n",
      "2022-06-15 07:46:56.190 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chester-big-book-group/ scraped successfully: Link number 33\n",
      "2022-06-15 07:46:56.430 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/\n",
      "2022-06-15 07:46:56.431 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/ scraped successfully: Link number 34\n",
      "2022-06-15 07:46:56.687 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/\n",
      "2022-06-15 07:46:56.688 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/ scraped successfully: Link number 35\n",
      "2022-06-15 07:46:56.965 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/\n",
      "2022-06-15 07:46:56.967 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/ scraped successfully: Link number 36\n",
      "2022-06-15 07:46:57.219 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/pali-womens-group/\n",
      "2022-06-15 07:46:57.221 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/pali-womens-group/ scraped successfully: Link number 37\n",
      "2022-06-15 07:46:57.458 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/ohua-group/\n",
      "2022-06-15 07:46:57.459 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/ohua-group/ scraped successfully: Link number 38\n",
      "2022-06-15 07:46:57.681 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/\n",
      "2022-06-15 07:46:57.682 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/ scraped successfully: Link number 39\n",
      "2022-06-15 07:47:02.687 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.711 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.715 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.720 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.726 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.745 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.746 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.747 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.749 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.753 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.754 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.755 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.756 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.761 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.762 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.762 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.764 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.768 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.769 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.770 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.772 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.775 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.776 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.777 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.778 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.782 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.784 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.785 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.787 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.790 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.791 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.792 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.793 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.804 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.805 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.806 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.807 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:02.814 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:02.814 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:02.815 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:02.816 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:02.819 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:40 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:03.087 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/malia-discussion/\n",
      "2022-06-15 07:47:03.091 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/malia-discussion/ scraped successfully: Link number 40\n",
      "2022-06-15 07:47:03.373 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/\n",
      "2022-06-15 07:47:03.373 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/ scraped successfully: Link number 41\n",
      "2022-06-15 07:47:03.612 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/\n",
      "2022-06-15 07:47:03.613 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/ scraped successfully: Link number 42\n",
      "2022-06-15 07:47:03.879 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-23-anoka/\n",
      "2022-06-15 07:47:03.880 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-23-anoka/ scraped successfully: Link number 43\n",
      "2022-06-15 07:47:04.107 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-20-anoka/\n",
      "2022-06-15 07:47:04.108 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-20-anoka/ scraped successfully: Link number 44\n",
      "2022-06-15 07:47:04.344 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-fever/\n",
      "2022-06-15 07:47:04.345 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-fever/ scraped successfully: Link number 45\n",
      "2022-06-15 07:47:04.594 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-2-anoka/\n",
      "2022-06-15 07:47:04.595 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-2-anoka/ scraped successfully: Link number 46\n",
      "2022-06-15 07:47:04.989 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-18-anoka/\n",
      "2022-06-15 07:47:04.990 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-18-anoka/ scraped successfully: Link number 47\n",
      "2022-06-15 07:47:05.260 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/\n",
      "2022-06-15 07:47:05.261 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/ scraped successfully: Link number 48\n",
      "2022-06-15 07:47:05.538 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/\n",
      "2022-06-15 07:47:05.539 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/ scraped successfully: Link number 49\n",
      "2022-06-15 07:47:10.547 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.559 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.562 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.564 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.571 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.587 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.589 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.591 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.593 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.599 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.600 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.602 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.605 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.610 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.611 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.612 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.613 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.616 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.620 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.621 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.622 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.625 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.626 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.626 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.628 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.633 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.634 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.635 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.637 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.640 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.641 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.642 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.644 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.647 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.648 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.650 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.651 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:10.654 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:10.655 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:10.656 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:10.657 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:10.659 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:50 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:10.929 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/wendover-group/\n",
      "2022-06-15 07:47:10.930 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/wendover-group/ scraped successfully: Link number 50\n",
      "2022-06-15 07:47:11.189 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-meeting-sparks/\n",
      "2022-06-15 07:47:11.190 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/big-book-meeting-sparks/ scraped successfully: Link number 51\n",
      "2022-06-15 07:47:11.460 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/eureka-group-south-main-street/\n",
      "2022-06-15 07:47:11.462 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/eureka-group-south-main-street/ scraped successfully: Link number 52\n",
      "2022-06-15 07:47:11.695 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/out-of-the-fog-out-of-the-bog-and-into-the-light/\n",
      "2022-06-15 07:47:11.697 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/out-of-the-fog-out-of-the-bog-and-into-the-light/ scraped successfully: Link number 53\n",
      "2022-06-15 07:47:11.941 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/keep-it-simple-franklin-tennessee/\n",
      "2022-06-15 07:47:11.942 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/keep-it-simple-franklin-tennessee/ scraped successfully: Link number 54\n",
      "2022-06-15 07:47:12.197 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/gratitude-and-hope-group/\n",
      "2022-06-15 07:47:12.198 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/gratitude-and-hope-group/ scraped successfully: Link number 55\n",
      "2022-06-15 07:47:12.442 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/franklin-group-tennessee/\n",
      "2022-06-15 07:47:12.443 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/franklin-group-tennessee/ scraped successfully: Link number 56\n",
      "2022-06-15 07:47:12.726 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fairview-group-fairview-tennessee/\n",
      "2022-06-15 07:47:12.728 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fairview-group-fairview-tennessee/ scraped successfully: Link number 57\n",
      "2022-06-15 07:47:12.998 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-out-west/\n",
      "2022-06-15 07:47:12.999 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-out-west/ scraped successfully: Link number 58\n",
      "2022-06-15 07:47:13.233 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/tuesday-night-mens-stag/\n",
      "2022-06-15 07:47:13.234 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/tuesday-night-mens-stag/ scraped successfully: Link number 59\n",
      "2022-06-15 07:47:18.238 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.246 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.247 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.248 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.249 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.252 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.253 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.254 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.255 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.258 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.258 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.259 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.260 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.265 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.265 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.266 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.267 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.273 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.280 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.282 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.284 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.289 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.290 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.291 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.293 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.304 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.305 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.308 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.309 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.314 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.315 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.316 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.317 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.321 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.322 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.322 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.324 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:18.327 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:18.328 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:18.329 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:18.330 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:18.332 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:60 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:18.598 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/surrender-to-win-group/\n",
      "2022-06-15 07:47:18.601 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/surrender-to-win-group/ scraped successfully: Link number 60\n",
      "2022-06-15 07:47:18.871 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bone-dry-group-buckeye-arizona/\n",
      "2022-06-15 07:47:18.872 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bone-dry-group-buckeye-arizona/ scraped successfully: Link number 61\n",
      "2022-06-15 07:47:19.133 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/black-canyon-city-aa-group/\n",
      "2022-06-15 07:47:19.134 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/black-canyon-city-aa-group/ scraped successfully: Link number 62\n",
      "2022-06-15 07:47:19.399 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/dry-dock-group-ruthven/\n",
      "2022-06-15 07:47:19.401 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/dry-dock-group-ruthven/ scraped successfully: Link number 63\n",
      "2022-06-15 07:47:19.676 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/williams-hall-group/\n",
      "2022-06-15 07:47:19.677 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/williams-hall-group/ scraped successfully: Link number 64\n",
      "2022-06-15 07:47:19.934 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/dewitt-meeting/\n",
      "2022-06-15 07:47:19.936 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/dewitt-meeting/ scraped successfully: Link number 65\n",
      "2022-06-15 07:47:20.179 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/t-g-i-s-group-alton/\n",
      "2022-06-15 07:47:20.179 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/t-g-i-s-group-alton/ scraped successfully: Link number 66\n",
      "2022-06-15 07:47:20.434 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/women-in-recovery-12-and-12-meeting/\n",
      "2022-06-15 07:47:20.436 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/women-in-recovery-12-and-12-meeting/ scraped successfully: Link number 67\n",
      "2022-06-15 07:47:20.697 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-morning-bbsg/\n",
      "2022-06-15 07:47:20.698 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-morning-bbsg/ scraped successfully: Link number 68\n",
      "2022-06-15 07:47:20.974 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/by-the-book-bb-12-step-study/\n",
      "2022-06-15 07:47:20.976 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/by-the-book-bb-12-step-study/ scraped successfully: Link number 69\n",
      "2022-06-15 07:47:25.984 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:25.996 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.000 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.004 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.010 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.024 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.026 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.028 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.033 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.042 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.048 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.049 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.051 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.055 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.057 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.057 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.059 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.064 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.065 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.066 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.068 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.075 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.076 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.077 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.079 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.082 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.083 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.084 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.086 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.090 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.092 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.093 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.095 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.099 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.100 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.101 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.102 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:26.105 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:26.106 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:26.106 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:26.108 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:26.111 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:70 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:26.365 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/battle-ground-aa-meeting/\n",
      "2022-06-15 07:47:26.366 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/battle-ground-aa-meeting/ scraped successfully: Link number 70\n",
      "2022-06-15 07:47:26.635 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/mens-spiritual-search-group/\n",
      "2022-06-15 07:47:26.637 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/mens-spiritual-search-group/ scraped successfully: Link number 71\n",
      "2022-06-15 07:47:26.907 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12-and-12-book-study-antioch-illinois/\n",
      "2022-06-15 07:47:26.909 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12-and-12-book-study-antioch-illinois/ scraped successfully: Link number 72\n",
      "2022-06-15 07:47:27.183 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/st-annes-elementary-school-saturdays/\n",
      "2022-06-15 07:47:27.184 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/st-annes-elementary-school-saturdays/ scraped successfully: Link number 73\n",
      "2022-06-15 07:47:27.440 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-wednesdays/\n",
      "2022-06-15 07:47:27.442 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-wednesdays/ scraped successfully: Link number 74\n",
      "2022-06-15 07:47:27.672 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-mondays/\n",
      "2022-06-15 07:47:27.673 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-mondays/ scraped successfully: Link number 75\n",
      "2022-06-15 07:47:27.926 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/the-market-street-group/\n",
      "2022-06-15 07:47:27.927 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/the-market-street-group/ scraped successfully: Link number 76\n",
      "2022-06-15 07:47:28.189 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/mens-meeting-cheyenne/\n",
      "2022-06-15 07:47:28.190 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/mens-meeting-cheyenne/ scraped successfully: Link number 77\n",
      "2022-06-15 07:47:28.501 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/group-1-at-300-club/\n",
      "2022-06-15 07:47:28.501 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/group-1-at-300-club/ scraped successfully: Link number 78\n",
      "2022-06-15 07:47:28.764 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/thursday-night-royal-meeting/\n",
      "2022-06-15 07:47:28.768 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/thursday-night-royal-meeting/ scraped successfully: Link number 79\n",
      "2022-06-15 07:47:33.778 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.805 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.808 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.810 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.813 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.819 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.820 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.822 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.824 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.828 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.829 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.830 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.832 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.835 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.836 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.837 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.839 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.843 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.847 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.848 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.850 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.854 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.855 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.857 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.858 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.862 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.863 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.864 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.866 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.870 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.872 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.872 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.877 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.894 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.899 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.900 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.902 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:33.906 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:33.906 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:33.907 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:33.908 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:33.911 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:80 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:34.163 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fellowship-in-recovery-cheyenne-wyoming/\n",
      "2022-06-15 07:47:34.164 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fellowship-in-recovery-cheyenne-wyoming/ scraped successfully: Link number 80\n",
      "2022-06-15 07:47:34.391 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/sun-fun-group/\n",
      "2022-06-15 07:47:34.392 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/sun-fun-group/ scraped successfully: Link number 81\n",
      "2022-06-15 07:47:34.641 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/eye-openers-group-cheyenne/\n",
      "2022-06-15 07:47:34.643 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/eye-openers-group-cheyenne/ scraped successfully: Link number 82\n",
      "2022-06-15 07:47:34.890 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/poplar-group/\n",
      "2022-06-15 07:47:34.892 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/poplar-group/ scraped successfully: Link number 83\n",
      "2022-06-15 07:47:35.150 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/eye-openers-group-beginners-meeting/\n",
      "2022-06-15 07:47:35.151 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/eye-openers-group-beginners-meeting/ scraped successfully: Link number 84\n",
      "2022-06-15 07:47:35.422 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/north-myrtle-beach-group/\n",
      "2022-06-15 07:47:35.423 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/north-myrtle-beach-group/ scraped successfully: Link number 85\n",
      "2022-06-15 07:47:35.695 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/love-and-tolerance-north-myrtle-beach/\n",
      "2022-06-15 07:47:35.696 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/love-and-tolerance-north-myrtle-beach/ scraped successfully: Link number 86\n",
      "2022-06-15 07:47:35.972 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/choices-group-ashville-pennsylvania/\n",
      "2022-06-15 07:47:35.973 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/choices-group-ashville-pennsylvania/ scraped successfully: Link number 87\n",
      "2022-06-15 07:47:36.211 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/oconee-group/\n",
      "2022-06-15 07:47:36.219 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/oconee-group/ scraped successfully: Link number 88\n",
      "2022-06-15 07:47:36.478 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/happy-joyous-and-free-group-ashley-pennsylvania/\n",
      "2022-06-15 07:47:36.480 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/happy-joyous-and-free-group-ashley-pennsylvania/ scraped successfully: Link number 89\n",
      "2022-06-15 07:47:41.485 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.491 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.492 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.493 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.495 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.506 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.507 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.508 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.510 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.523 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.526 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.545 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.555 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.564 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.570 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.573 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.577 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.581 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.583 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.584 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.586 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.591 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.592 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.593 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.596 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.600 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.601 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.602 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.604 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.609 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.612 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.613 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.616 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.624 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.625 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.632 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.634 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:41.638 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:41.639 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:41.640 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:41.642 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:41.644 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:90 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:41.910 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/higher-learners-group/\n",
      "2022-06-15 07:47:41.913 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/higher-learners-group/ scraped successfully: Link number 90\n",
      "2022-06-15 07:47:42.191 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fountain-roc-group/\n",
      "2022-06-15 07:47:42.192 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fountain-roc-group/ scraped successfully: Link number 91\n",
      "2022-06-15 07:47:42.458 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/new-year-group-bayhealth/\n",
      "2022-06-15 07:47:42.460 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/new-year-group-bayhealth/ scraped successfully: Link number 92\n",
      "2022-06-15 07:47:42.730 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/turning-point-group-altoona-pennsylvania/\n",
      "2022-06-15 07:47:42.731 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/turning-point-group-altoona-pennsylvania/ scraped successfully: Link number 93\n",
      "2022-06-15 07:47:42.987 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/going-to-any-lengths-dover/\n",
      "2022-06-15 07:47:42.988 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/going-to-any-lengths-dover/ scraped successfully: Link number 94\n",
      "2022-06-15 07:47:43.229 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/toast-masters-group/\n",
      "2022-06-15 07:47:43.230 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/toast-masters-group/ scraped successfully: Link number 95\n",
      "2022-06-15 07:47:43.471 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fresh-air-dover/\n",
      "2022-06-15 07:47:43.473 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fresh-air-dover/ scraped successfully: Link number 96\n",
      "2022-06-15 07:47:43.735 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bridge-to-life-group/\n",
      "2022-06-15 07:47:43.737 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bridge-to-life-group/ scraped successfully: Link number 97\n",
      "2022-06-15 07:47:43.993 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/amagansett-saturday-night/\n",
      "2022-06-15 07:47:43.995 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/amagansett-saturday-night/ scraped successfully: Link number 98\n",
      "2022-06-15 07:47:44.262 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturdays-in-the-park/\n",
      "2022-06-15 07:47:44.264 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturdays-in-the-park/ scraped successfully: Link number 99\n",
      "2022-06-15 07:47:49.270 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.281 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.284 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.288 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.295 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.307 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.309 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.311 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.314 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.319 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.321 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.322 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.324 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.329 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.329 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.330 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.333 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.336 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.337 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.339 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.341 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.345 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.346 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.347 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.348 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.353 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.354 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.355 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.360 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.365 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.366 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.367 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.369 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.378 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.378 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.379 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.381 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 07:47:49.384 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 07:47:49.385 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 07:47:49.386 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 07:47:49.387 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 07:47:49.389 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:100 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 07:47:49.391 | INFO     | __main__:scrape:38 - <<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "scrape(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Old Functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped to 13180\n",
    "new_csv_file = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state',\n",
    "           'zip_code', 'day', 'time', 'info']\n",
    "\n",
    "\n",
    "def csv_writer(row_data, csv_filename, headers=None):\n",
    "    with open(csv_filename, 'w') as f:\n",
    "        if headers is not None:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            writer.writeheader(headers)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            fieldnames = [k for k in row_data.keys()]\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='|')\n",
    "            writer.writeheader(fieldnames)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "csv_writer(row_data1, csv_filename, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c957a87b781b1e0e6e953571db6a2c4b79eab3b2dd8ca59b5a7c957ad56688c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('scrapevenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
