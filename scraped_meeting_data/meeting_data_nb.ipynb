{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "import lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_parser(csv_reader, header: str):\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = {\n",
    "    'AK': 'Alaska',\n",
    "    'AL': 'Alabama',\n",
    "    'AR': 'Arkansas',\n",
    "    'AZ': 'Arizona',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DC': 'District of Columbia',\n",
    "    'DE': 'Delaware',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'IA': 'Iowa',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MD': 'Maryland',\n",
    "    'ME': 'Maine',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MO': 'Missouri',\n",
    "    'MS': 'Mississippi',\n",
    "    'MT': 'Montana',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'NE': 'Nebraska',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NV': 'Nevada',\n",
    "    'NY': 'New York',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VA': 'Virginia',\n",
    "    'VT': 'Vermont',\n",
    "    'WA': 'Washington',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WV': 'West Virginia',\n",
    "    'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take all the data at each key[value][index] position and create a dict for each\n",
    "# index postion, then append that dict to an array of dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open CSV file to append data to:\n",
    "# with open('new_meeting_data.csv', 'w') as f:\n",
    "#     csv_writer = csv.DictWriter(f, field_names=list(data.keys()))\n",
    "\n",
    "# program start:\n",
    "# start = time.time()\n",
    "\n",
    "# end of program timer\n",
    "# end = time.time()\n",
    "# t = end - start\n",
    "# print(f\"Program took: {t} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = './meetings.csv'\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = get_links(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The func can only take one link at a time so that each row gets \n",
    "# created in the correct order.\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "def meeting_row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    row['name'] = item0['name']\n",
    "    row['address'] = item0['address']\n",
    "    row['city'] = item0['city']\n",
    "    row['state'] = item0['state']\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = '|'.join(item1['day'])\n",
    "    row['time'] = '|'.join(item1['time'])\n",
    "    row['info'] = '|'.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    return row\n",
    "\n",
    "\n",
    "    # empty list to append individual data to\n",
    "    dict_list = []\n",
    "\n",
    "    # Now loop through the CSV file and append to the data list:\n",
    "    for k, v in data.items():\n",
    "        row_data = {k: v}\n",
    "        dict_list.append(row_data)\n",
    "    return dict_list\n",
    "\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    def get_address_data(soup):\n",
    "        \n",
    "        try:\n",
    "            address_tag = soup.address\n",
    "            address = address_tag.contents[1]\n",
    "            \n",
    "            meeting_name = soup.find(\n",
    "                'div', class_='fui-card-body').find(class_='weight-300')\n",
    "            name = meeting_name.contents[1]\n",
    "            \n",
    "            city_tag = meeting_name.find_next('a')\n",
    "            city = city_tag.contents[0]\n",
    "            \n",
    "            state_tag = city_tag.find_next('a')\n",
    "            state = state_tag.contents[0]\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "            \n",
    "        except IndexError as ie:\n",
    "            print(f\"Index Error: {ie}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            \n",
    "\n",
    "        \n",
    "    def get_table_data(soup):\n",
    "        try:\n",
    "            info_table = soup.find('table', class_='table fui-table')\n",
    "            # obtain all the columns from <th>\n",
    "            headers = []\n",
    "            for i in info_table.find_all('th'):\n",
    "                title = i.text\n",
    "                headers.append(title.lower())\n",
    "\n",
    "                # now create a dataframe:\n",
    "            df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        \n",
    "            # Now create the foor loop to fill dataframe\n",
    "            # a row is under the <tr> tags and data under the <td> tags\n",
    "            for j in info_table.find_all('tr')[1:]:\n",
    "                # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "                #     print(\"No info table found\")\n",
    "                row_data = j.find_all('td')\n",
    "                row = [i.text for i in row_data]\n",
    "                length = len(df)\n",
    "                df.loc[length] = row\n",
    "\n",
    "            # data['day'].append(df['day'].to_list())\n",
    "            # data['time'].append(df['time'].to_list())\n",
    "            # data['info'].append(df['info'].to_list())\n",
    "            day = df['day'].to_list()\n",
    "            time = df['time'].to_list()\n",
    "            info = df['info'].to_list()\n",
    "\n",
    "            # now return data\n",
    "            return {'day': day, 'time': time, 'info': info}\n",
    "        \n",
    "        except AttributeError as ae:\n",
    "            print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "            return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "\n",
    "\n",
    "    def meeting_row_parser(item0, item1):\n",
    "        \"\"\"\n",
    "        :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "        the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "        :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "        access the data -> Keys: 'day' - 'time' - 'info'\n",
    "        \n",
    "        create a final dictionary that will be used to store the information in the database as one row. \n",
    "        I will need to join the list items to create one string with a | seperating each item so I can \n",
    "        split the string when retrieving the data.\n",
    "        \"\"\"\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            row['name'] = 'name'\n",
    "            row['address'] = 'address'\n",
    "            row['city'] = 'city'\n",
    "            row['state'] = 'state'\n",
    "\n",
    "        # now add item1 to the row data\n",
    "        row['day'] = ' | '.join(item1['day'])\n",
    "        row['time'] = ' | '.join(item1['time'])\n",
    "        row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "        # now return the row data dictionary\n",
    "        return row\n",
    "    \n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = meeting_row_parser(d[0], d[1])\n",
    "    \n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use the scraper function to scrape each link in that list\n",
    "# and store the data in the d variable for dict\n",
    "row_data = []\n",
    "for link in link_list[1:]:\n",
    "    row = meeting_data_scraper(link)\n",
    "    # data = add_row_data_to_list(row)\n",
    "    row_data.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n"
     ]
    }
   ],
   "source": [
    "print(len(row_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from multiprocessing.pool import Pool\n",
    "executor = ThreadPoolExecutor(max_workers=2)\n",
    "\n",
    "\n",
    "def create_soup_obj_from_page(pages):\n",
    "    soup_obj = []\n",
    "    for page in pages:\n",
    "        soup = bs(page.text, 'lxml')\n",
    "        soup_obj.append(soup)\n",
    "    return soup_obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch pages from each link:\n",
    "def fetch_page_data(link):\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "link_list = get_links(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "map() missing 1 required positional argument: 'iterable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000032vscode-remote?line=0'>1</a>\u001b[0m soup_obj \u001b[39m=\u001b[39m Pool\u001b[39m.\u001b[39;49mmap(fetch_page_data, link_list[\u001b[39m1\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: map() missing 1 required positional argument: 'iterable'"
     ]
    }
   ],
   "source": [
    "soup_obj = Pool.map(fetch_page_data, link_list[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CREATE FUNCTION THAT WILL GET THE PAGE CONTENTS AND CREATE SOUP OBJ\n",
    "def create_soup_obj_from_contents(link):\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# TODO: CREATE FUNCTION THAT WILL RETRIEVE THE ADDRESS DATA AND RETURN DICT\n",
    "def get_address_data(soup):\n",
    "\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "\n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "\n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "\n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "\n",
    "    except IndexError as ie:\n",
    "        print(f\"Index Error: {ie}\")\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "\n",
    "# TODO: CREATE FUNCTION THAT WILL RETRIEVE THE DETAILS DATA FROM AN HTML TABLE\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "\n",
    "# TODO: PASS THOSE TWO DICTS TO A FUNCTION THAT WILL PARSE AND COMBINE THEM:\n",
    "def meeting_row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    return row\n",
    "\n",
    "# TODO: CREATE A FUNCTION THAT WILL WRITE EACH ITEM IN THE ROW_DATA LIST TO A CSV FILE\n",
    "# SAVING THE ROW VALUES IN THEIR KEY'S COLUMN:\n",
    "def convert_row_data_to_csv(row_data):\n",
    "    with open('meeting_details.csv', 'w') as csvfile:\n",
    "        for d in row_data:\n",
    "            csvwriter = csv.DictWriter(csvfile, delimiter=',', field_names=list(d.keys()))\n",
    "            csvwriter.writerow(d)\n",
    "            \n",
    "# TODO: GET THE LINKED LIST FROM THE CSV FILE IN meetings.csv\n",
    "csv_filename = './meetings.csv'\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "# TODO: RETRIEVE THAT COMBINED DICT, AND APPEND TO AN EMPTY LIST CALLED ROW DATA\n",
    "\n",
    "\"\"\"\n",
    "functionality: \n",
    "\n",
    "1) Create a for loop, that will take each link in the linked list and it will, \n",
    "create a soup obj and return the contents:\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_data_to_list(data: dict):\n",
    "    # empty list to append individual data to\n",
    "    dict_list = []\n",
    "\n",
    "    # Now loop through the CSV file and append to the data list:\n",
    "    for k, v in data.items():\n",
    "        row_data = {k: v}\n",
    "        dict_list.append(row_data)\n",
    "    return dict_list\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c957a87b781b1e0e6e953571db6a2c4b79eab3b2dd8ca59b5a7c957ad56688c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('scrapevenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
