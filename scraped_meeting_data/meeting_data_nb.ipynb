{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "import lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_parser(csv_reader, header: str):\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "states = {\n",
    "    'AK': 'Alaska',\n",
    "    'AL': 'Alabama',\n",
    "    'AR': 'Arkansas',\n",
    "    'AZ': 'Arizona',\n",
    "    'CA': 'California',\n",
    "    'CO': 'Colorado',\n",
    "    'CT': 'Connecticut',\n",
    "    'DC': 'District of Columbia',\n",
    "    'DE': 'Delaware',\n",
    "    'FL': 'Florida',\n",
    "    'GA': 'Georgia',\n",
    "    'HI': 'Hawaii',\n",
    "    'IA': 'Iowa',\n",
    "    'ID': 'Idaho',\n",
    "    'IL': 'Illinois',\n",
    "    'IN': 'Indiana',\n",
    "    'KS': 'Kansas',\n",
    "    'KY': 'Kentucky',\n",
    "    'LA': 'Louisiana',\n",
    "    'MA': 'Massachusetts',\n",
    "    'MD': 'Maryland',\n",
    "    'ME': 'Maine',\n",
    "    'MI': 'Michigan',\n",
    "    'MN': 'Minnesota',\n",
    "    'MO': 'Missouri',\n",
    "    'MS': 'Mississippi',\n",
    "    'MT': 'Montana',\n",
    "    'NC': 'North Carolina',\n",
    "    'ND': 'North Dakota',\n",
    "    'NE': 'Nebraska',\n",
    "    'NH': 'New Hampshire',\n",
    "    'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico',\n",
    "    'NV': 'Nevada',\n",
    "    'NY': 'New York',\n",
    "    'OH': 'Ohio',\n",
    "    'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'RI': 'Rhode Island',\n",
    "    'SC': 'South Carolina',\n",
    "    'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee',\n",
    "    'TX': 'Texas',\n",
    "    'UT': 'Utah',\n",
    "    'VA': 'Virginia',\n",
    "    'VT': 'Vermont',\n",
    "    'WA': 'Washington',\n",
    "    'WI': 'Wisconsin',\n",
    "    'WV': 'West Virginia',\n",
    "    'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Take all the data at each key[value][index] position and create a dict for each\n",
    "# index postion, then append that dict to an array of dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Open CSV file to append data to:\n",
    "# with open('new_meeting_data.csv', 'w') as f:\n",
    "#     csv_writer = csv.DictWriter(f, field_names=list(data.keys()))\n",
    "\n",
    "# program start:\n",
    "# start = time.time()\n",
    "\n",
    "# end of program timer\n",
    "# end = time.time()\n",
    "# t = end - start\n",
    "# print(f\"Program took: {t} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = './meetings.csv'\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = get_links(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The func can only take one link at a time so that each row gets \n",
    "# created in the correct order.\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "def meeting_row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    row['name'] = item0['name']\n",
    "    row['address'] = item0['address']\n",
    "    row['city'] = item0['city']\n",
    "    row['state'] = item0['state']\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = '|'.join(item1['day'])\n",
    "    row['time'] = '|'.join(item1['time'])\n",
    "    row['info'] = '|'.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    return row\n",
    "\n",
    "\n",
    "    # empty list to append individual data to\n",
    "    dict_list = []\n",
    "\n",
    "    # Now loop through the CSV file and append to the data list:\n",
    "    for k, v in data.items():\n",
    "        row_data = {k: v}\n",
    "        dict_list.append(row_data)\n",
    "    return dict_list\n",
    "\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    def get_address_data(soup):\n",
    "        \n",
    "        try:\n",
    "            address_tag = soup.address\n",
    "            address = address_tag.contents[1]\n",
    "            \n",
    "            meeting_name = soup.find(\n",
    "                'div', class_='fui-card-body').find(class_='weight-300')\n",
    "            name = meeting_name.contents[1]\n",
    "            \n",
    "            city_tag = meeting_name.find_next('a')\n",
    "            city = city_tag.contents[0]\n",
    "            \n",
    "            state_tag = city_tag.find_next('a')\n",
    "            state = state_tag.contents[0]\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "            \n",
    "        except IndexError as ie:\n",
    "            print(f\"Index Error: {ie}\")\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        \n",
    "    def get_table_data(soup):\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        try:\n",
    "            # Now create the foor loop to fill dataframe\n",
    "            # a row is under the <tr> tags and data under the <td> tags\n",
    "            for j in info_table.find_all('tr')[1:]:\n",
    "                # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "                #     print(\"No info table found\")\n",
    "                row_data = j.find_all('td')\n",
    "                row = [i.text for i in row_data]\n",
    "                length = len(df)\n",
    "                df.loc[length] = row\n",
    "\n",
    "            # data['day'].append(df['day'].to_list())\n",
    "            # data['time'].append(df['time'].to_list())\n",
    "            # data['info'].append(df['info'].to_list())\n",
    "            day = df['day'].to_list()\n",
    "            time = df['time'].to_list()\n",
    "            info = df['info'].to_list()\n",
    "\n",
    "            # now return data\n",
    "            return {'day': day, 'time': time, 'info': info}\n",
    "        \n",
    "        except AttributeError as ae:\n",
    "            print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "            return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "\n",
    "\n",
    "    def meeting_row_parser(item0, item1):\n",
    "        \"\"\"\n",
    "        :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "        the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "        :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "        access the data -> Keys: 'day' - 'time' - 'info'\n",
    "        \n",
    "        create a final dictionary that will be used to store the information in the database as one row. \n",
    "        I will need to join the list items to create one string with a | seperating each item so I can \n",
    "        split the string when retrieving the data.\n",
    "        \"\"\"\n",
    "        row = {}\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "\n",
    "        # now add item1 to the row data\n",
    "        row['day'] = ' | '.join(item1['day'])\n",
    "        row['time'] = ' | '.join(item1['time'])\n",
    "        row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "        # now return the row data dictionary\n",
    "        return row\n",
    "    \n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = meeting_row_parser(d[0], d[1])\n",
    "    \n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Error: list index out of range\n",
      "Index Error: list index out of range\n",
      "Index Error: list index out of range\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000018vscode-remote?line=2'>3</a>\u001b[0m row_data \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000018vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m link \u001b[39min\u001b[39;00m link_list[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000018vscode-remote?line=4'>5</a>\u001b[0m     row \u001b[39m=\u001b[39m meeting_data_scraper(link)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000018vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m# data = add_row_data_to_list(row)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000018vscode-remote?line=6'>7</a>\u001b[0m     row_data\u001b[39m.\u001b[39mappend(row)\n",
      "\u001b[1;32m/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 8'\u001b[0m in \u001b[0;36mmeeting_data_scraper\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=122'>123</a>\u001b[0m \u001b[39m# Create two dicts with the following keys\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=123'>124</a>\u001b[0m address_dict \u001b[39m=\u001b[39m get_address_data(soup)\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=124'>125</a>\u001b[0m details_dict \u001b[39m=\u001b[39m get_table_data(soup)\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=126'>127</a>\u001b[0m d \u001b[39m=\u001b[39m [address_dict, details_dict]\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=127'>128</a>\u001b[0m row_data \u001b[39m=\u001b[39m meeting_row_parser(d[\u001b[39m0\u001b[39m], d[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32m/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 8'\u001b[0m in \u001b[0;36mmeeting_data_scraper.<locals>.get_table_data\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=64'>65</a>\u001b[0m \u001b[39m# obtain all the columns from <th>\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=65'>66</a>\u001b[0m headers \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=66'>67</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m info_table\u001b[39m.\u001b[39;49mfind_all(\u001b[39m'\u001b[39m\u001b[39mth\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=67'>68</a>\u001b[0m     title \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39mtext\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000008vscode-remote?line=68'>69</a>\u001b[0m     headers\u001b[39m.\u001b[39mappend(title\u001b[39m.\u001b[39mlower())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "# Now use the scraper function to scrape each link in that list\n",
    "# and store the data in the d variable for dict\n",
    "row_data = []\n",
    "for link in link_list[1:]:\n",
    "    row = meeting_data_scraper(link)\n",
    "    # data = add_row_data_to_list(row)\n",
    "    row_data.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': ' Chillicothe Big Book Study', 'address': ' 291 South Paint Street ', 'city': 'Chillicothe', 'state': 'Ohio', 'day': 'Wednesday', 'time': '7:00 pm - 8:00 pm', 'info': 'Big Book, Discussion, Open Meeting, Wheelchair Access'}]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_data_to_list(data: dict):\n",
    "    # empty list to append individual data to\n",
    "    dict_list = []\n",
    "\n",
    "    # Now loop through the CSV file and append to the data list:\n",
    "    for k, v in data.items():\n",
    "        row_data = {k: v}\n",
    "        dict_list.append(row_data)\n",
    "    return dict_list"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c957a87b781b1e0e6e953571db6a2c4b79eab3b2dd8ca59b5a7c957ad56688c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('scrapevenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
