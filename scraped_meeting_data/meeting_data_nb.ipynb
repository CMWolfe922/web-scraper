{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from loguru import logger\n",
    "import time\n",
    "# this is where any files or directories/path variables go:\n",
    "csv_filename = './meetings.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP A LOGURU LOGGER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = './data/meeting_scraper.log'\n",
    "rotation=\"100 MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(log_file_path, rotation=rotation, level=\"INFO\", format=\"[<green>{time: MMM D YYYY HH:mm:ss:SSSS}</>] | <level>{message}</>\", backtrace=True, diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A BASIC SCRAPER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_list_from_column_data(csv_filename, column):\n",
    "    \"\"\"\n",
    "    :description: Create a list from a CSV column's data. To do this I will use the with open()\n",
    "    context manager and open the csv file that was passed to this function as an argument.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :param csv_filename:-> This is the name of the CSV file containing the column data I want to extract\n",
    "    to create a list.\n",
    "    :param column:-> This is the column name or number to extract data from in the CSV file.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :return:-> A list of strings/integers/float/datetypes from a CSV file.\n",
    "    \"\"\"\n",
    "    # now create an empty list to append data to:\n",
    "    data_list = []\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        _column = next(csv_reader)\n",
    "        columnIndex = _column.index(column)\n",
    "        # loop through CSV file and append to address_list\n",
    "        for line in csv_reader:\n",
    "            all_data = line[columnIndex]\n",
    "            data_list.append(all_data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_soup_data(link):\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"Retrieved soup_data for link: {}\", link)\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL PARSE THE ROW DATA AND STORE IT \n",
    "# IN A DICTIONARY. THAT DICTIONARY CAN THEN BE INSERTED INTO \n",
    "# A LIST OF DICTIONARIES CALLED ROW_LIST BUT TO DO THIS THE PARSER\n",
    "# HAS TO JOIN LIST ITEMS INTO ONE LONG STRING SO EACH ROW HAS THE \n",
    "# SAME NUMBER OF COLUMNS:\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "\n",
    "# VERSION 1\n",
    "def row_parser1(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n",
    "\n",
    "# VERSION 2: \n",
    "def row_parser2(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "            # now add item1 to the row data\n",
    "            row['day'] = ' | '.join(item1['day'])\n",
    "            row['time'] = ' | '.join(item1['time'])\n",
    "            row['info'] = ' | '.join(item1['info'])\n",
    "            # now return the row data dictionary\n",
    "            return row\n",
    "        except Exception as e:\n",
    "            print(f'{e}')\n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "        for k, v in row.items():\n",
    "            if v is not None:\n",
    "                pass\n",
    "            else:\n",
    "                v = k\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE 'MAIN LOGICAL FUNCTION' THIS FUNCTION WILL COMBINE THE \n",
    "# get_address_data, get_table_data, and meeting_row_parser FUNCTIONS.\n",
    "# THAT WAY I CAN EXECUTE ALL OF THE FUNCTIONS IN ONE CALL.\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    # Get Soup Data\n",
    "    soup = fetch_soup_data(link)\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = row_parser1(d[0], d[1])\n",
    "\n",
    "    logger.info(\"[+] Main Meeting Dart Scraper Worked\")\n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return it.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_column_data(csv_filename, column):\n",
    "    \"\"\"\n",
    "    :description: Create a list from a CSV column's data. To do this I will use the with open()\n",
    "    context manager and open the csv file that was passed to this function as an argument.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :param csv_filename:-> This is the name of the CSV file containing the column data I want to extract\n",
    "    to create a list.\n",
    "    :param column:-> This is the column name or number to extract data from in the CSV file.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :return:-> A list of strings/integers/float/datetypes from a CSV file.\n",
    "    \"\"\"\n",
    "    # now create an empty list to append data to:\n",
    "    data_list = []\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        _column = next(csv_reader)\n",
    "        columnIndex = _column.index(column)\n",
    "        # loop through CSV file and append to address_list\n",
    "        for line in csv_reader:\n",
    "            all_data = line[columnIndex]\n",
    "            data_list.append(all_data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def fetch_soup_data(link):\n",
    "    \"\"\"\n",
    "    Basic function that takes a link from link_list and accepts it as an argument, \n",
    "    then uses the requests.get method to return an html page. Then uses BeautifulSoup \n",
    "    to parse the pages' text using lxml\n",
    "    \"\"\"\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the address data from a soup object that gets past to it in a \n",
    "    for loop. \n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: A dictionary containing the address data components: {address, name, city, state}\n",
    "\n",
    "    ::: I need to figure out how to add zip_code to this :::\n",
    "    \"\"\"\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "\n",
    "\n",
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the meeting details data from a soup object that gets past to it in a \n",
    "    for loop. The data is extracted from an html table. Each row that returns a list is joined together to make \n",
    "    one string so it can be saved to a csv file.\n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: a dictionary with the days the meeting runs, the time, and info about the meeting: \n",
    "    {day, time, info}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        row['zip_code'] = ''\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = ' | '.join(item1['day'])\n",
    "    row['time'] = ' | '.join(item1['time'])\n",
    "    row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv_file = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "def csv_writer(row_data, csv_filename, headers=None):\n",
    "    with open(csv_filename, 'a') as f:\n",
    "        if headers is not None:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            # writer.writeheader(headers)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            # fieldnames = [k for k in row_data.keys()]\n",
    "            # writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='|')\n",
    "            # writer.writeheader(fieldnames)\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC COMPONENTS BUILT..\n",
    "\n",
    "<hr>\n",
    "\n",
    "## CREATE LINK LISTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "10000\n",
      "3588\n"
     ]
    }
   ],
   "source": [
    "# whole list of links\n",
    "link_list = create_list_from_column_data(csv_filename, 'link')\n",
    "\n",
    "# shortened list: first 1000\n",
    "links_1000 = link_list[:1000]\n",
    "\n",
    "# shortened list: first 500\n",
    "links_500 = link_list[:500]\n",
    "\n",
    "links_100 = link_list[:100]\n",
    "\n",
    "####################################################################################\n",
    "# chunk the links into bunches of 10,000\n",
    "link_list1 = link_list[:10000]\n",
    "link_list2 = link_list[10000:20000]\n",
    "link_list3 = link_list[20000:30000]\n",
    "link_list4 = link_list[30000:]\n",
    "\n",
    "print(len(link_list1))\n",
    "print(len(link_list2))\n",
    "print(len(link_list3))\n",
    "print(len(link_list4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BUILD TWO FUNCTIONS PUTTING ALL THE ABOVE COMPONENTS TOGETHER\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_data_scraper(soup_data):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        rows = []\n",
    "        for soup in soup_data:\n",
    "            # Create two dicts with the following keys\n",
    "            address_dict = get_address_data(soup)\n",
    "            details_dict = get_table_data(soup)\n",
    "            logger.info(\"[+] Address and table data parsed\")\n",
    "            d = [address_dict, details_dict]\n",
    "            row_data = row_parser(d[0], d[1])\n",
    "            rows.append(row_data)\n",
    "        logger.info(\"++++++++ SCRAPED ALL THE LINKS +++++++++ \")\n",
    "        return rows\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: The list that is returned from the get_links function. Then it is passed to \n",
    "    scrape function that is used inside this function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup_data = scrape(link_list)\n",
    "        rows = soup_data_scraper(soup_data)\n",
    "        return rows\n",
    "\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        time.sleep(10)\n",
    "        logger.error(\"{} Request Exception occured\", re)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception while scraping link list\", e)\n",
    "\n",
    "new_csv_filename = './meeting_details.csv'\n",
    "\n",
    "def scrape(link_list, csv_file):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    row_data = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 500 links, write the row data to the new CSV file\n",
    "                for soup in soup_data:\n",
    "                    row = soup_data_scraper(soup)\n",
    "                    row_data.append(row)\n",
    "                # check that row_data has 500 items:\n",
    "                    if len(row_data) == 500:\n",
    "                        # after each soup item is parsed and the results appended to row_data, append row data to csv\n",
    "                        csv_writer(row_data, csv_file, headers)\n",
    "                        # Then clear both soup and row data lists\n",
    "                        soup_data.clear()\n",
    "                        row_data.clear()\n",
    "                        # log the success of the script\n",
    "                        logger.info(\"[+] Count is divisible by 500:{} | Writing ROW_DATA to [{}] | soup_data and row_data cleared!\", count, csv_file)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link_list, csv_file=None):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    # row_data = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state',\n",
    "               'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\n",
    "                \"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 50 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 500 links, write the row data to the new CSV file\n",
    "                # if len(soup_data) == 500:\n",
    "                rows = [soup_data_scraper(soup) for soup in soup_data]\n",
    "                logger.info(\"[+] list comprehension worked creating rows list !!!!!!!!!!!!\")\n",
    "                return rows\n",
    "                    # row_data.append(row)\n",
    "                # check that row_data has 500 items:\n",
    "                    # if len(row_data) == 500:\n",
    "                    #     # after each soup item is parsed and the results appended to row_data, append row data to csv\n",
    "                    #     csv_writer(row_data, csv_file, headers)\n",
    "                    #     # Then clear both soup and row data lists\n",
    "                    #     soup_data.clear()\n",
    "                    #     row_data.clear()\n",
    "                    #     # log the success of the script\n",
    "                    #     logger.info(\n",
    "                    #         \"[+] Count is divisible by 500:{} | Writing ROW_DATA to [{}] | soup_data and row_data cleared!\", count, csv_file)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-11 18:40:54.216 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-11 18:40:54.217 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 0\n",
      "2022-06-11 18:40:54.474 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-11 18:40:54.475 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 1\n",
      "2022-06-11 18:40:54.687 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-11 18:40:54.688 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 2\n",
      "2022-06-11 18:40:54.883 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-11 18:40:54.884 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 3\n",
      "2022-06-11 18:40:55.079 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-11 18:40:55.080 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 4\n",
      "2022-06-11 18:40:55.280 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-11 18:40:55.281 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 5\n",
      "2022-06-11 18:40:55.475 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-11 18:40:55.476 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 6\n",
      "2022-06-11 18:40:55.681 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-11 18:40:55.681 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 7\n",
      "2022-06-11 18:40:55.886 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-11 18:40:55.887 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 8\n",
      "2022-06-11 18:40:56.084 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-11 18:40:56.085 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 9\n",
      "2022-06-11 18:40:56.279 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-11 18:40:56.280 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 10\n",
      "2022-06-11 18:40:56.474 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-11 18:40:56.475 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 11\n",
      "2022-06-11 18:40:56.674 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-11 18:40:56.675 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 12\n",
      "2022-06-11 18:40:56.869 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-11 18:40:56.871 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 13\n",
      "2022-06-11 18:40:57.067 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-11 18:40:57.068 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 14\n",
      "2022-06-11 18:40:57.261 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-11 18:40:57.262 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 15\n",
      "2022-06-11 18:40:57.464 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-11 18:40:57.464 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 16\n",
      "2022-06-11 18:40:57.674 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-11 18:40:57.675 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 17\n",
      "2022-06-11 18:40:57.877 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-11 18:40:57.878 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 18\n",
      "2022-06-11 18:40:58.072 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-11 18:40:58.073 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 19\n",
      "2022-06-11 18:40:58.283 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-11 18:40:58.284 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 20\n",
      "2022-06-11 18:40:58.488 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-11 18:40:58.489 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 21\n",
      "2022-06-11 18:40:58.690 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-11 18:40:58.691 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 22\n",
      "2022-06-11 18:40:58.885 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-11 18:40:58.886 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 23\n",
      "2022-06-11 18:40:59.096 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/athens-group/\n",
      "2022-06-11 18:40:59.097 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/athens-group/ scraped successfully: Link number 24\n",
      "2022-06-11 18:40:59.304 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/\n",
      "2022-06-11 18:40:59.305 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/ scraped successfully: Link number 25\n",
      "2022-06-11 18:40:59.517 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/triangle-group-online/\n",
      "2022-06-11 18:40:59.518 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/triangle-group-online/ scraped successfully: Link number 26\n",
      "2022-06-11 18:40:59.726 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/\n",
      "2022-06-11 18:40:59.727 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/ scraped successfully: Link number 27\n",
      "2022-06-11 18:40:59.932 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/\n",
      "2022-06-11 18:40:59.933 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/ scraped successfully: Link number 28\n",
      "2022-06-11 18:41:00.144 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/discussion-anaheim/\n",
      "2022-06-11 18:41:00.145 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/discussion-anaheim/ scraped successfully: Link number 29\n",
      "2022-06-11 18:41:00.357 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/in-the-hall-group/\n",
      "2022-06-11 18:41:00.358 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/in-the-hall-group/ scraped successfully: Link number 30\n",
      "2022-06-11 18:41:00.575 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/shining-light-group/\n",
      "2022-06-11 18:41:00.576 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/shining-light-group/ scraped successfully: Link number 31\n",
      "2022-06-11 18:41:00.788 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/\n",
      "2022-06-11 18:41:00.789 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/ scraped successfully: Link number 32\n",
      "2022-06-11 18:41:00.996 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chester-big-book-group/\n",
      "2022-06-11 18:41:00.997 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chester-big-book-group/ scraped successfully: Link number 33\n",
      "2022-06-11 18:41:01.213 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/\n",
      "2022-06-11 18:41:01.214 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/ scraped successfully: Link number 34\n",
      "2022-06-11 18:41:01.435 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/\n",
      "2022-06-11 18:41:01.435 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/ scraped successfully: Link number 35\n",
      "2022-06-11 18:41:01.647 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/\n",
      "2022-06-11 18:41:01.648 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/ scraped successfully: Link number 36\n",
      "2022-06-11 18:41:01.863 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/pali-womens-group/\n",
      "2022-06-11 18:41:01.863 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/pali-womens-group/ scraped successfully: Link number 37\n",
      "2022-06-11 18:41:02.073 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/ohua-group/\n",
      "2022-06-11 18:41:02.074 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/ohua-group/ scraped successfully: Link number 38\n",
      "2022-06-11 18:41:02.284 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/\n",
      "2022-06-11 18:41:02.285 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/ scraped successfully: Link number 39\n",
      "2022-06-11 18:41:02.500 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/malia-discussion/\n",
      "2022-06-11 18:41:02.501 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/malia-discussion/ scraped successfully: Link number 40\n",
      "2022-06-11 18:41:02.713 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/\n",
      "2022-06-11 18:41:02.714 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/ scraped successfully: Link number 41\n",
      "2022-06-11 18:41:02.921 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/\n",
      "2022-06-11 18:41:02.922 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/ scraped successfully: Link number 42\n",
      "2022-06-11 18:41:03.127 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-23-anoka/\n",
      "2022-06-11 18:41:03.128 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-23-anoka/ scraped successfully: Link number 43\n",
      "2022-06-11 18:41:03.341 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-20-anoka/\n",
      "2022-06-11 18:41:03.342 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-20-anoka/ scraped successfully: Link number 44\n",
      "2022-06-11 18:41:03.559 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-fever/\n",
      "2022-06-11 18:41:03.560 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-fever/ scraped successfully: Link number 45\n",
      "2022-06-11 18:41:03.767 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-2-anoka/\n",
      "2022-06-11 18:41:03.768 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-2-anoka/ scraped successfully: Link number 46\n",
      "2022-06-11 18:41:04.075 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-18-anoka/\n",
      "2022-06-11 18:41:04.076 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-18-anoka/ scraped successfully: Link number 47\n",
      "2022-06-11 18:41:04.439 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/\n",
      "2022-06-11 18:41:04.440 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/ scraped successfully: Link number 48\n",
      "2022-06-11 18:41:04.651 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/\n",
      "2022-06-11 18:41:04.652 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/ scraped successfully: Link number 49\n",
      "2022-06-11 18:41:09.658 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.660 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.661 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.664 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.666 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.667 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.671 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.672 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.673 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.674 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.675 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.676 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.677 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.677 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.678 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.679 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.679 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.680 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.681 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.682 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.683 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.684 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.685 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.685 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.686 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.687 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.688 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.689 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.689 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.691 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.692 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.692 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.693 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.694 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.694 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.695 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.696 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.696 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.697 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.699 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.700 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.701 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.703 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.703 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.704 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.705 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.706 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.707 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.708 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.708 | ERROR    | __main__:soup_data_scraper:25 - 'Doctype' object has no attribute 'address': Exception raised\n",
      "2022-06-11 18:41:09.709 | INFO     | __main__:scrape:25 - [+] list comprehension worked creating rows list !!!!!!!!!!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape(links_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped to 13180\n",
    "new_csv_file = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "def csv_writer(row_data, csv_filename, headers=None):\n",
    "    with open(csv_filename, 'w') as f:\n",
    "        if headers is not None:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            writer.writeheader(headers)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            fieldnames = [k for k in row_data.keys()]\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='|')\n",
    "            writer.writeheader(fieldnames)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "csv_writer(row_data1, csv_filename, headers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'row_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/blackhood/github/myrepos/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000036?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(row_data))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'row_data' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for >>: 'builtin_function_or_method' and 'str'. Did you mean \"print(<message>, file=<output_stream>)\"?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000028vscode-remote?line=0'>1</a>\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mTesting\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/blackbox/projects/github/web-scraper/scraped_meeting_data/meeting_data_nb.ipynb#ch0000028vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39;49m \u001b[39m>>\u001b[39;49mmsg\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for >>: 'builtin_function_or_method' and 'str'. Did you mean \"print(<message>, file=<output_stream>)\"?"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c957a87b781b1e0e6e953571db6a2c4b79eab3b2dd8ca59b5a7c957ad56688c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('scrapevenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
