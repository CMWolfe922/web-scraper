{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from loguru import logger\n",
    "import time\n",
    "# this is where any files or directories/path variables go:\n",
    "csv_filename = './meetings.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP A LOGURU LOGGER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = './data/meeting_scraper.log'\n",
    "rotation=\"100 MB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.add(log_file_path, rotation=rotation, level=\"INFO\", format=\"[<green>{time: MMM D YYYY HH:mm:ss:SSSS}</>] | <level>{message}</>\", backtrace=True, diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A BASIC SCRAPER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_list_from_column_data(csv_filename, column):\n",
    "    \"\"\"\n",
    "    :description: Create a list from a CSV column's data. To do this I will use the with open()\n",
    "    context manager and open the csv file that was passed to this function as an argument.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :param csv_filename:-> This is the name of the CSV file containing the column data I want to extract\n",
    "    to create a list.\n",
    "    :param column:-> This is the column name or number to extract data from in the CSV file.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :return:-> A list of strings/integers/float/datetypes from a CSV file.\n",
    "    \"\"\"\n",
    "    # now create an empty list to append data to:\n",
    "    data_list = []\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        _column = next(csv_reader)\n",
    "        columnIndex = _column.index(column)\n",
    "        # loop through CSV file and append to address_list\n",
    "        for line in csv_reader:\n",
    "            all_data = line[columnIndex]\n",
    "            data_list.append(all_data)\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_soup_data(link):\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"Retrieved soup_data for link: {}\", link)\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL PARSE THE ROW DATA AND STORE IT \n",
    "# IN A DICTIONARY. THAT DICTIONARY CAN THEN BE INSERTED INTO \n",
    "# A LIST OF DICTIONARIES CALLED ROW_LIST BUT TO DO THIS THE PARSER\n",
    "# HAS TO JOIN LIST ITEMS INTO ONE LONG STRING SO EACH ROW HAS THE \n",
    "# SAME NUMBER OF COLUMNS:\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "\n",
    "# VERSION 1\n",
    "def parse_dicts(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {'name':[], 'address':[], 'city':[], 'state':[], 'zip_code':[], 'day':[], 'time':[], 'info':[]}\n",
    "    try:\n",
    "\n",
    "        try:\n",
    "            row['name'].append(item0['name'])\n",
    "            row['address'].append(item0['address'])\n",
    "            row['city'].append(item0['city'])\n",
    "            row['state'].append(item0['state'])\n",
    "            row['zip_code'].append('00000')\n",
    "        except Exception as e:\n",
    "            logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "            print(e)\n",
    "            row['name'].append('name')\n",
    "            row['address'].append('address')\n",
    "            row['city'].append('city')\n",
    "            row['state'].append('state')\n",
    "            row['zip_code'].append('zip_code')\n",
    "\n",
    "        # now add item1 to the row data\n",
    "        row['day'].append(' | '.join(item1['day']))\n",
    "        row['time'].append(' | '.join(item1['time']))\n",
    "        row['info'].append(' | '.join(item1['info']))\n",
    "\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "        return row\n",
    "    except Exception as e:\n",
    "        logger.error(\"Exception Raised: {}\", e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE 'MAIN LOGICAL FUNCTION' THIS FUNCTION WILL COMBINE THE \n",
    "# get_address_data, get_table_data, and meeting_row_parser FUNCTIONS.\n",
    "# THAT WAY I CAN EXECUTE ALL OF THE FUNCTIONS IN ONE CALL.\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    # Get Soup Data\n",
    "    soup = fetch_soup_data(link)\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = row_parser1(d[0], d[1])\n",
    "\n",
    "    logger.info(\"[+] Main Meeting Dart Scraper Worked\")\n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return it.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_column_data(csv_filename, column):\n",
    "    \"\"\"\n",
    "    :description: Create a list from a CSV column's data. To do this I will use the with open()\n",
    "    context manager and open the csv file that was passed to this function as an argument.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :param csv_filename:-> This is the name of the CSV file containing the column data I want to extract\n",
    "    to create a list.\n",
    "    :param column:-> This is the column name or number to extract data from in the CSV file.\n",
    "    ----------------------------------------------------------------------------------------------------\n",
    "    :return:-> A list of strings/integers/float/datetypes from a CSV file.\n",
    "    \"\"\"\n",
    "    # now create an empty list to append data to:\n",
    "    data_list = []\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        _column = next(csv_reader)\n",
    "        columnIndex = _column.index(column)\n",
    "        # loop through CSV file and append to address_list\n",
    "        for line in csv_reader:\n",
    "            all_data = line[columnIndex]\n",
    "            data_list.append(all_data)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def fetch_soup_data(link):\n",
    "    \"\"\"\n",
    "    Basic function that takes a link from link_list and accepts it as an argument, \n",
    "    then uses the requests.get method to return an html page. Then uses BeautifulSoup \n",
    "    to parse the pages' text using lxml\n",
    "    \"\"\"\n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "    logger.info(\"got soup data for link: {}\", link)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the address data from a soup object that gets past to it in a \n",
    "    for loop. \n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: A dictionary containing the address data components: {address, name, city, state}\n",
    "\n",
    "    ::: I need to figure out how to add zip_code to this :::\n",
    "    \"\"\"\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "        \n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "        \n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "        \n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        logger.info(\"[+] Address data retrieved\")\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "        \n",
    "    except IndexError as ie:\n",
    "        logger.error(\"[-] UnboundError occured {} \", ie)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundLocalError occured: {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            logger.error(\"[-] UnboundError occured {} \", ule)\n",
    "\n",
    "\n",
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "    \"\"\"\n",
    "    :description: Extracts the meeting details data from a soup object that gets past to it in a \n",
    "    for loop. The data is extracted from an html table. Each row that returns a list is joined together to make \n",
    "    one string so it can be saved to a csv file.\n",
    "\n",
    "    :param soup: This is a BeautifulSoup object that has been pulled from a soup_data list that was \n",
    "    created using the fetch_soup_data function and stored into a soup_data list.\n",
    "\n",
    "    :returns: a dictionary with the days the meeting runs, the time, and info about the meeting: \n",
    "    {day, time, info}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "    \n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        logger.info(\"[+] Meeting Details Retrieved\")\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "    \n",
    "    except AttributeError as ae:\n",
    "        logger.error(\"info_table.find_all('tr') failed: {}\", ae)\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "        row['zip_code'] = '00000'\n",
    "        logger.info(\"[+] Row Data Parsed\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"[-] Row Data Raised Exception: {}\", e)\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    row['day'] = '|'.join(item1['day'].replace(',', '-'))\n",
    "    row['time'] = '|'.join(item1['time'].replace(',', '-'))\n",
    "    row['info'] = '|'.join(item1['info'].replace(',', '-'))\n",
    "\n",
    "    # now return the row data dictionary\n",
    "    \n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv_file = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state', 'zip_code', 'day', 'time', 'info']\n",
    "def csv_writer(row_data, csv_filename, headers=None):\n",
    "    with open(csv_filename, 'a') as f:\n",
    "        if headers is not None:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            # writer.writeheader(headers)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            # fieldnames = [k for k in row_data.keys()]\n",
    "            # writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='|')\n",
    "            # writer.writeheader(fieldnames)\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASIC COMPONENTS BUILT..\n",
    "\n",
    "<hr>\n",
    "\n",
    "## CREATE LINK LISTS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole list of links\n",
    "link_list = create_list_from_column_data(csv_filename, 'link')\n",
    "\n",
    "# shortened list: first 1000\n",
    "links_1000 = link_list[:1000]\n",
    "\n",
    "# shortened list: first 500\n",
    "links_500 = link_list[:500]\n",
    "\n",
    "links_100 = link_list[:100]\n",
    "\n",
    "####################################################################################\n",
    "# chunk the links into bunches of 10,000\n",
    "link_list1 = link_list[:10000]\n",
    "link_list2 = link_list[10000:20000]\n",
    "link_list3 = link_list[20000:30000]\n",
    "link_list4 = link_list[30000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BUILD TWO FUNCTIONS PUTTING ALL THE ABOVE COMPONENTS TOGETHER\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_list_scraper(soup_data):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        rows = {'name':[], 'address':[], 'city':[], 'state':[], 'zip_code':[], 'day':[], 'time':[], 'info':[]}\n",
    "        for soup in soup_data:\n",
    "            # Create two dicts with the following keys\n",
    "            address_dict = get_address_data(soup)\n",
    "            details_dict = get_table_data(soup)\n",
    "            logger.info(\"[+] Address and table data parsed\")\n",
    "            d = [address_dict, details_dict]\n",
    "            row_data = parse_dicts(d[0], d[1])\n",
    "            rows['name'].append(row_data['name'])\n",
    "            rows['address'].append(row_data['address'])\n",
    "            rows['city'].append(row_data['city'])\n",
    "            rows['state'].append(row_data['state'])\n",
    "            rows['zip_code'].append(row_data['zip_code'])\n",
    "            rows['day'].append(row_data['day'])\n",
    "            rows['time'].append(row_data['time'])\n",
    "            rows['info'].append(row_data['info'])\n",
    "\n",
    "        logger.info(\"++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \")\n",
    "        return rows\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)\n",
    "        \n",
    "\n",
    "def single_soup_scraper(soup):\n",
    "    \"\"\"\n",
    "    :param soup_data: This is the list of soup data that was returned by the scrape function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create two dicts with the following keys\n",
    "        address_dict = get_address_data(soup)\n",
    "        details_dict = get_table_data(soup)\n",
    "        d = [address_dict, details_dict]\n",
    "        row = row_parser(d[0], d[1])\n",
    "        logger.info(\n",
    "            \"[+] Parsed Address and Table Data | Row [{}] Created\", row)\n",
    "        return row\n",
    "\n",
    "    except IndexError as ie:\n",
    "        logger.error(\"{}: List Exhausted. No more Soup Items\", ie)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception raised\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: The list that is returned from the get_links function. Then it is passed to \n",
    "    scrape function that is used inside this function. \n",
    "\n",
    "    :returns: a list of dictionaries that will get stored to the variable row since each dict \n",
    "    creates a row of data for the csv file. \n",
    "    \"\"\"\n",
    "    try:\n",
    "        soup_data = scrape(link_list)\n",
    "        rows = soup_list_scraper(soup_data)\n",
    "        return rows\n",
    "\n",
    "    except requests.exceptions.RequestException as re:\n",
    "        time.sleep(10)\n",
    "        logger.error(\"{} Request Exception occured\", re)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"{}: Exception while scraping link list\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link_list, csv_file):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    row_data = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state',\n",
    "               'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\n",
    "                \"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 500 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 500 links, write the row data to the new CSV file\n",
    "                for soup in soup_data:\n",
    "                    row = single_soup_scraper(soup)\n",
    "                    row_data.append(row)\n",
    "                # check that row_data has 500 items:\n",
    "                    if len(row_data) == 500:\n",
    "                        # after each soup item is parsed and the results appended to row_data, append row data to csv\n",
    "                        csv_writer(row_data, csv_file, headers)\n",
    "                        # Then clear both soup and row data lists\n",
    "                        soup_data.clear()\n",
    "                        row_data.clear()\n",
    "                        # log the success of the script\n",
    "                        logger.info(\n",
    "                            \"[+] Count is divisible by 500:{} | Writing ROW_DATA to [{}] | soup_data and row_data cleared!\", count, csv_file)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_csv_filename = './meeting_details.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(link_list):\n",
    "    \"\"\"\n",
    "    :param link_list: A list of links to scrape and get the soup data from the site, \n",
    "    parse the data and append the soup data to the soup_data list. \n",
    "    \"\"\"\n",
    "    # Two empty lists. One for soup_data and another to store row_data\n",
    "    dataframes = []\n",
    "    soup_data = []\n",
    "    # The headers for the csv_file\n",
    "    headers = [\"name\", 'address', 'city', 'state',\n",
    "               'zip_code', 'day', 'time', 'info']\n",
    "    try:\n",
    "        count = 0\n",
    "        for link in link_list:\n",
    "            soup = fetch_soup_data(link)\n",
    "            soup_data.append(soup)\n",
    "            logger.info(\n",
    "                \"[+] {} scraped successfully: Link number {}\", link, count)\n",
    "            count += 1\n",
    "            if count % 10 == 0:\n",
    "                time.sleep(5)\n",
    "                # Every 10 links, write the row data to the new CSV file\n",
    "                rows = soup_list_scraper(soup_data)\n",
    "                row_df = pd.DataFrame.from_dict(rows)\n",
    "                dataframes.append(row_df)\n",
    "                soup_data.clear()\n",
    "                logger.info(\n",
    "                            \"[+] Count is divisible by 10:{} | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\", count)\n",
    "                # check that row_data has 500 items:\n",
    "                    # if len(dataframes) > 0:\n",
    "                    #     # after each soup item is parsed and the results appended to row_data, append row data to csv\n",
    "                    #     csv_writer(row_data, csv_file, headers)\n",
    "                    #     # Then clear both soup and row data lists\n",
    "                    #     soup_data.clear()\n",
    "                    #     row_data.clear()\n",
    "                    #     # log the success of the script\n",
    "        print(dataframes)\n",
    "        logger.info(\"<<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\")\n",
    "    except IndexError as e:\n",
    "        logger.error(\"{}: List Exhausted. No more links to scrape.\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 06:34:25.384 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-15 06:34:25.386 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 0\n",
      "2022-06-15 06:34:25.752 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-15 06:34:25.753 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 1\n",
      "2022-06-15 06:34:26.107 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-15 06:34:26.109 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 2\n",
      "2022-06-15 06:34:26.406 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-15 06:34:26.407 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 3\n",
      "2022-06-15 06:34:26.787 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-15 06:34:26.788 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 4\n",
      "2022-06-15 06:34:27.201 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-15 06:34:27.203 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 5\n",
      "2022-06-15 06:34:27.586 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-15 06:34:27.587 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 6\n",
      "2022-06-15 06:34:27.921 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-15 06:34:27.922 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 7\n",
      "2022-06-15 06:34:28.204 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-15 06:34:28.205 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 8\n",
      "2022-06-15 06:34:28.516 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-15 06:34:28.518 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 9\n",
      "2022-06-15 06:34:33.526 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.539 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.543 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.546 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.554 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.564 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.566 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.568 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.571 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.578 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.579 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.581 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.583 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.586 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.587 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.588 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.590 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.597 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.598 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.599 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.600 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.603 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.604 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.605 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.607 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.615 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.616 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.617 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.619 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.622 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.623 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.624 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.626 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.630 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.631 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.632 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.634 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:33.638 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:33.639 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:33.640 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:33.641 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:34:33.643 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:10 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:34:33.984 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-15 06:34:33.985 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 10\n",
      "2022-06-15 06:34:35.042 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-15 06:34:35.044 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 11\n",
      "2022-06-15 06:34:35.368 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/\n",
      "2022-06-15 06:34:35.369 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-first-capital-group/ scraped successfully: Link number 12\n",
      "2022-06-15 06:34:35.762 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/\n",
      "2022-06-15 06:34:35.763 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chillicothe-big-book-study/ scraped successfully: Link number 13\n",
      "2022-06-15 06:34:36.087 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/\n",
      "2022-06-15 06:34:36.089 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/waverly-pike-county-group/ scraped successfully: Link number 14\n",
      "2022-06-15 06:34:36.419 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/\n",
      "2022-06-15 06:34:36.420 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/one-step-at-a-time-group-waverly/ scraped successfully: Link number 15\n",
      "2022-06-15 06:34:36.763 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/searsport-group/\n",
      "2022-06-15 06:34:36.765 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/searsport-group/ scraped successfully: Link number 16\n",
      "2022-06-15 06:34:37.180 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/east-millinocket-group/\n",
      "2022-06-15 06:34:37.181 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/east-millinocket-group/ scraped successfully: Link number 17\n",
      "2022-06-15 06:34:37.545 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/\n",
      "2022-06-15 06:34:37.547 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/togus-va-speaker-meeting/ scraped successfully: Link number 18\n",
      "2022-06-15 06:34:37.946 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/\n",
      "2022-06-15 06:34:37.948 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/serenity-group-augusta/ scraped successfully: Link number 19\n",
      "2022-06-15 06:34:42.954 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:42.991 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:42.993 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:42.994 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:42.997 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.002 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.004 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.007 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.009 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.014 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.015 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.016 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.017 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.021 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.023 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.024 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.026 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.031 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.032 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.033 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.034 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.037 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.038 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.039 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.040 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.045 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.046 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.046 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.048 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.051 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.052 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.054 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.055 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.059 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.060 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.061 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.063 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:43.067 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:43.069 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:43.070 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:43.071 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:34:43.072 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:20 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:34:43.449 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/looney-toons-group/\n",
      "2022-06-15 06:34:43.450 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/looney-toons-group/ scraped successfully: Link number 20\n",
      "2022-06-15 06:34:43.758 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/\n",
      "2022-06-15 06:34:43.760 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12th-step-being-a-sponsor/ scraped successfully: Link number 21\n",
      "2022-06-15 06:34:44.091 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/\n",
      "2022-06-15 06:34:44.092 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-of-life-group-atlanta-texas/ scraped successfully: Link number 22\n",
      "2022-06-15 06:34:44.441 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/\n",
      "2022-06-15 06:34:44.442 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/grupo-libertad-10-de-junio/ scraped successfully: Link number 23\n",
      "2022-06-15 06:34:45.378 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/athens-group/\n",
      "2022-06-15 06:34:45.379 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/athens-group/ scraped successfully: Link number 24\n",
      "2022-06-15 06:34:46.604 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/\n",
      "2022-06-15 06:34:46.606 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/contoocook-keep-it-simple-group/ scraped successfully: Link number 25\n",
      "2022-06-15 06:34:47.224 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/triangle-group-online/\n",
      "2022-06-15 06:34:47.225 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/triangle-group-online/ scraped successfully: Link number 26\n",
      "2022-06-15 06:34:48.328 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/\n",
      "2022-06-15 06:34:48.329 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/aa-happy-thoughts-group/ scraped successfully: Link number 27\n",
      "2022-06-15 06:34:49.488 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/\n",
      "2022-06-15 06:34:49.489 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/esperanza-de-vivir/ scraped successfully: Link number 28\n",
      "2022-06-15 06:34:50.609 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/discussion-anaheim/\n",
      "2022-06-15 06:34:50.610 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/discussion-anaheim/ scraped successfully: Link number 29\n",
      "2022-06-15 06:34:55.618 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.632 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.635 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.638 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.648 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.658 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.660 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.663 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.666 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.684 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.685 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.686 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.688 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.692 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.693 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.694 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.696 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.704 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.705 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.706 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.708 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.711 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.712 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.713 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.716 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.721 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.724 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.725 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.727 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.731 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.733 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.733 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.735 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.749 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.750 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.751 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.752 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:34:55.762 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:34:55.763 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:34:55.764 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:34:55.764 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:34:55.766 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:30 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:34:57.061 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/in-the-hall-group/\n",
      "2022-06-15 06:34:57.062 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/in-the-hall-group/ scraped successfully: Link number 30\n",
      "2022-06-15 06:34:57.420 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/shining-light-group/\n",
      "2022-06-15 06:34:57.421 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/shining-light-group/ scraped successfully: Link number 31\n",
      "2022-06-15 06:34:58.367 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/\n",
      "2022-06-15 06:34:58.368 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/carry-the-message-anaheim-california/ scraped successfully: Link number 32\n",
      "2022-06-15 06:34:59.366 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/chester-big-book-group/\n",
      "2022-06-15 06:34:59.367 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/chester-big-book-group/ scraped successfully: Link number 33\n",
      "2022-06-15 06:35:00.272 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/\n",
      "2022-06-15 06:35:00.273 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bills-babes-womens-book-study/ scraped successfully: Link number 34\n",
      "2022-06-15 06:35:01.249 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/\n",
      "2022-06-15 06:35:01.251 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/big-book-study-anaheim-california/ scraped successfully: Link number 35\n",
      "2022-06-15 06:35:02.388 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/\n",
      "2022-06-15 06:35:02.390 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-live-honolulu/ scraped successfully: Link number 36\n",
      "2022-06-15 06:35:03.360 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/pali-womens-group/\n",
      "2022-06-15 06:35:03.362 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/pali-womens-group/ scraped successfully: Link number 37\n",
      "2022-06-15 06:35:04.307 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/ohua-group/\n",
      "2022-06-15 06:35:04.308 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/ohua-group/ scraped successfully: Link number 38\n",
      "2022-06-15 06:35:05.249 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/\n",
      "2022-06-15 06:35:05.250 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/new-beginnings-womens-meeting/ scraped successfully: Link number 39\n",
      "2022-06-15 06:35:10.258 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.282 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.286 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.289 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.294 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.310 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.312 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.313 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.315 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.320 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.321 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.322 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.324 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.328 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.329 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.330 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.332 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.334 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.335 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.336 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.338 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.341 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.342 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.349 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.350 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.353 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.354 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.355 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.357 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.360 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.361 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.362 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.363 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.374 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.375 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.375 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.377 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:10.381 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:10.382 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:10.382 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:10.383 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:35:10.385 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:40 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:35:11.235 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/malia-discussion/\n",
      "2022-06-15 06:35:11.237 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/malia-discussion/ scraped successfully: Link number 40\n",
      "2022-06-15 06:35:12.320 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/\n",
      "2022-06-15 06:35:12.321 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/huntsville-group-church-avenue/ scraped successfully: Link number 41\n",
      "2022-06-15 06:35:12.654 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/\n",
      "2022-06-15 06:35:12.655 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-26-group-anoka-minnesota/ scraped successfully: Link number 42\n",
      "2022-06-15 06:35:13.024 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-23-anoka/\n",
      "2022-06-15 06:35:13.025 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-23-anoka/ scraped successfully: Link number 43\n",
      "2022-06-15 06:35:13.361 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-20-anoka/\n",
      "2022-06-15 06:35:13.362 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-20-anoka/ scraped successfully: Link number 44\n",
      "2022-06-15 06:35:14.575 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-night-fever/\n",
      "2022-06-15 06:35:14.576 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-night-fever/ scraped successfully: Link number 45\n",
      "2022-06-15 06:35:14.916 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-2-anoka/\n",
      "2022-06-15 06:35:14.918 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-2-anoka/ scraped successfully: Link number 46\n",
      "2022-06-15 06:35:15.259 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/squad-18-anoka/\n",
      "2022-06-15 06:35:15.260 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/squad-18-anoka/ scraped successfully: Link number 47\n",
      "2022-06-15 06:35:16.238 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/\n",
      "2022-06-15 06:35:16.239 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/language-of-the-heart-fallon/ scraped successfully: Link number 48\n",
      "2022-06-15 06:35:17.097 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/\n",
      "2022-06-15 06:35:17.099 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/speaker-and-birthday-meeting/ scraped successfully: Link number 49\n",
      "2022-06-15 06:35:22.107 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.118 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.122 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.124 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.133 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.150 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.151 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.153 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.156 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.161 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.162 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.164 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.166 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.170 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.172 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.173 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.175 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.180 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.181 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.181 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.183 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.189 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.191 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.192 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.194 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.198 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.199 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.200 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.202 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.208 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.209 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.210 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.212 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.216 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.217 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.218 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.219 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:22.223 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:22.224 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:22.225 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:22.226 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:35:22.227 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:50 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:35:23.481 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/wendover-group/\n",
      "2022-06-15 06:35:23.482 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/wendover-group/ scraped successfully: Link number 50\n",
      "2022-06-15 06:35:24.373 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/big-book-meeting-sparks/\n",
      "2022-06-15 06:35:24.375 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/big-book-meeting-sparks/ scraped successfully: Link number 51\n",
      "2022-06-15 06:35:25.323 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/eureka-group-south-main-street/\n",
      "2022-06-15 06:35:25.324 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/eureka-group-south-main-street/ scraped successfully: Link number 52\n",
      "2022-06-15 06:35:26.447 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/out-of-the-fog-out-of-the-bog-and-into-the-light/\n",
      "2022-06-15 06:35:26.449 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/out-of-the-fog-out-of-the-bog-and-into-the-light/ scraped successfully: Link number 53\n",
      "2022-06-15 06:35:27.649 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/keep-it-simple-franklin-tennessee/\n",
      "2022-06-15 06:35:27.650 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/keep-it-simple-franklin-tennessee/ scraped successfully: Link number 54\n",
      "2022-06-15 06:35:28.057 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/gratitude-and-hope-group/\n",
      "2022-06-15 06:35:28.058 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/gratitude-and-hope-group/ scraped successfully: Link number 55\n",
      "2022-06-15 06:35:29.116 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/franklin-group-tennessee/\n",
      "2022-06-15 06:35:29.117 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/franklin-group-tennessee/ scraped successfully: Link number 56\n",
      "2022-06-15 06:35:30.138 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fairview-group-fairview-tennessee/\n",
      "2022-06-15 06:35:30.140 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fairview-group-fairview-tennessee/ scraped successfully: Link number 57\n",
      "2022-06-15 06:35:30.495 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/way-out-west/\n",
      "2022-06-15 06:35:30.497 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/way-out-west/ scraped successfully: Link number 58\n",
      "2022-06-15 06:35:30.835 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/tuesday-night-mens-stag/\n",
      "2022-06-15 06:35:30.836 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/tuesday-night-mens-stag/ scraped successfully: Link number 59\n",
      "2022-06-15 06:35:35.842 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.879 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.881 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.882 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.884 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.889 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.891 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.892 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.893 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.898 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.899 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.900 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.902 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.908 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.910 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.911 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.917 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.924 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.926 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.926 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.928 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.932 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.934 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.935 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.937 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.947 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.948 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.948 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.950 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.956 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.957 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.957 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.959 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.962 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.963 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.964 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.965 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:35.968 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:35.970 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:35.971 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:35.971 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:35:35.975 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:60 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:35:36.298 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/surrender-to-win-group/\n",
      "2022-06-15 06:35:36.300 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/surrender-to-win-group/ scraped successfully: Link number 60\n",
      "2022-06-15 06:35:37.243 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bone-dry-group-buckeye-arizona/\n",
      "2022-06-15 06:35:37.245 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bone-dry-group-buckeye-arizona/ scraped successfully: Link number 61\n",
      "2022-06-15 06:35:38.328 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/black-canyon-city-aa-group/\n",
      "2022-06-15 06:35:38.329 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/black-canyon-city-aa-group/ scraped successfully: Link number 62\n",
      "2022-06-15 06:35:39.361 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/dry-dock-group-ruthven/\n",
      "2022-06-15 06:35:39.362 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/dry-dock-group-ruthven/ scraped successfully: Link number 63\n",
      "2022-06-15 06:35:39.685 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/williams-hall-group/\n",
      "2022-06-15 06:35:39.686 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/williams-hall-group/ scraped successfully: Link number 64\n",
      "2022-06-15 06:35:40.977 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/dewitt-meeting/\n",
      "2022-06-15 06:35:40.978 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/dewitt-meeting/ scraped successfully: Link number 65\n",
      "2022-06-15 06:35:41.330 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/t-g-i-s-group-alton/\n",
      "2022-06-15 06:35:41.331 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/t-g-i-s-group-alton/ scraped successfully: Link number 66\n",
      "2022-06-15 06:35:41.649 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/women-in-recovery-12-and-12-meeting/\n",
      "2022-06-15 06:35:41.650 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/women-in-recovery-12-and-12-meeting/ scraped successfully: Link number 67\n",
      "2022-06-15 06:35:42.497 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturday-morning-bbsg/\n",
      "2022-06-15 06:35:42.498 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturday-morning-bbsg/ scraped successfully: Link number 68\n",
      "2022-06-15 06:35:42.841 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/by-the-book-bb-12-step-study/\n",
      "2022-06-15 06:35:42.842 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/by-the-book-bb-12-step-study/ scraped successfully: Link number 69\n",
      "2022-06-15 06:35:47.850 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.865 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.869 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.874 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.880 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.887 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.889 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.891 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.896 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.903 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.905 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.907 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.909 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.913 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.914 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.914 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.915 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.920 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.921 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.922 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.924 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.934 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.935 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.936 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.938 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.943 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.944 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.945 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.946 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.951 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.952 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.953 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.955 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.959 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.964 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.965 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.968 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:35:47.972 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:35:47.974 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:35:47.975 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:35:47.976 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:35:47.977 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:70 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:35:48.978 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/battle-ground-aa-meeting/\n",
      "2022-06-15 06:35:48.980 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/battle-ground-aa-meeting/ scraped successfully: Link number 70\n",
      "2022-06-15 06:35:49.972 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/mens-spiritual-search-group/\n",
      "2022-06-15 06:35:49.973 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/mens-spiritual-search-group/ scraped successfully: Link number 71\n",
      "2022-06-15 06:35:51.225 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/12-and-12-book-study-antioch-illinois/\n",
      "2022-06-15 06:35:51.227 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/12-and-12-book-study-antioch-illinois/ scraped successfully: Link number 72\n",
      "2022-06-15 06:35:51.580 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/st-annes-elementary-school-saturdays/\n",
      "2022-06-15 06:35:51.581 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/st-annes-elementary-school-saturdays/ scraped successfully: Link number 73\n",
      "2022-06-15 06:35:52.545 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-wednesdays/\n",
      "2022-06-15 06:35:52.546 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-wednesdays/ scraped successfully: Link number 74\n",
      "2022-06-15 06:35:53.580 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-mondays/\n",
      "2022-06-15 06:35:53.581 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/emmanuel-lutheran-church-mondays/ scraped successfully: Link number 75\n",
      "2022-06-15 06:35:54.515 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/the-market-street-group/\n",
      "2022-06-15 06:35:54.516 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/the-market-street-group/ scraped successfully: Link number 76\n",
      "2022-06-15 06:35:56.556 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/mens-meeting-cheyenne/\n",
      "2022-06-15 06:35:56.558 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/mens-meeting-cheyenne/ scraped successfully: Link number 77\n",
      "2022-06-15 06:35:57.485 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/group-1-at-300-club/\n",
      "2022-06-15 06:35:57.491 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/group-1-at-300-club/ scraped successfully: Link number 78\n",
      "2022-06-15 06:35:57.893 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/thursday-night-royal-meeting/\n",
      "2022-06-15 06:35:57.894 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/thursday-night-royal-meeting/ scraped successfully: Link number 79\n",
      "2022-06-15 06:36:02.902 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.930 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.932 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.934 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.938 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.945 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.947 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.948 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.950 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.956 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.957 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.957 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.959 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.962 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.963 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.964 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.965 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.969 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.970 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.971 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.973 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.979 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.981 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.982 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.984 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.989 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.990 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.990 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:02.992 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:02.996 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:02.997 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:02.998 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:03.001 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:03.015 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:03.016 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:03.021 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:03.027 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:03.030 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:03.031 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:03.032 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:03.033 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:36:03.034 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:80 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:36:05.868 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fellowship-in-recovery-cheyenne-wyoming/\n",
      "2022-06-15 06:36:05.869 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fellowship-in-recovery-cheyenne-wyoming/ scraped successfully: Link number 80\n",
      "2022-06-15 06:36:06.194 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/sun-fun-group/\n",
      "2022-06-15 06:36:06.196 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/sun-fun-group/ scraped successfully: Link number 81\n",
      "2022-06-15 06:36:07.631 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/eye-openers-group-cheyenne/\n",
      "2022-06-15 06:36:07.633 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/eye-openers-group-cheyenne/ scraped successfully: Link number 82\n",
      "2022-06-15 06:36:08.824 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/poplar-group/\n",
      "2022-06-15 06:36:08.826 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/poplar-group/ scraped successfully: Link number 83\n",
      "2022-06-15 06:36:09.776 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/eye-openers-group-beginners-meeting/\n",
      "2022-06-15 06:36:09.777 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/eye-openers-group-beginners-meeting/ scraped successfully: Link number 84\n",
      "2022-06-15 06:36:10.172 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/north-myrtle-beach-group/\n",
      "2022-06-15 06:36:10.173 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/north-myrtle-beach-group/ scraped successfully: Link number 85\n",
      "2022-06-15 06:36:11.305 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/love-and-tolerance-north-myrtle-beach/\n",
      "2022-06-15 06:36:11.306 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/love-and-tolerance-north-myrtle-beach/ scraped successfully: Link number 86\n",
      "2022-06-15 06:36:12.324 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/choices-group-ashville-pennsylvania/\n",
      "2022-06-15 06:36:12.326 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/choices-group-ashville-pennsylvania/ scraped successfully: Link number 87\n",
      "2022-06-15 06:36:13.739 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/oconee-group/\n",
      "2022-06-15 06:36:13.741 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/oconee-group/ scraped successfully: Link number 88\n",
      "2022-06-15 06:36:14.056 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/happy-joyous-and-free-group-ashley-pennsylvania/\n",
      "2022-06-15 06:36:14.057 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/happy-joyous-and-free-group-ashley-pennsylvania/ scraped successfully: Link number 89\n",
      "2022-06-15 06:36:19.062 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.080 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.085 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.089 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.098 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.115 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.117 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.123 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.126 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.143 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.144 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.146 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.149 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.153 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.155 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.155 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.157 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.161 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.162 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.163 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.164 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.170 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.171 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.171 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.173 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.177 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.178 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.179 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.181 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.184 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.185 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.186 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.188 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.196 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.197 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.198 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.200 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:19.203 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:19.204 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:19.205 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:19.206 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:36:19.207 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:90 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:36:20.159 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/higher-learners-group/\n",
      "2022-06-15 06:36:20.161 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/higher-learners-group/ scraped successfully: Link number 90\n",
      "2022-06-15 06:36:21.351 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fountain-roc-group/\n",
      "2022-06-15 06:36:21.352 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fountain-roc-group/ scraped successfully: Link number 91\n",
      "2022-06-15 06:36:22.999 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/new-year-group-bayhealth/\n",
      "2022-06-15 06:36:23.000 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/new-year-group-bayhealth/ scraped successfully: Link number 92\n",
      "2022-06-15 06:36:23.328 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/turning-point-group-altoona-pennsylvania/\n",
      "2022-06-15 06:36:23.330 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/turning-point-group-altoona-pennsylvania/ scraped successfully: Link number 93\n",
      "2022-06-15 06:36:24.256 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/going-to-any-lengths-dover/\n",
      "2022-06-15 06:36:24.257 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/going-to-any-lengths-dover/ scraped successfully: Link number 94\n",
      "2022-06-15 06:36:24.572 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/toast-masters-group/\n",
      "2022-06-15 06:36:24.574 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/toast-masters-group/ scraped successfully: Link number 95\n",
      "2022-06-15 06:36:25.743 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/fresh-air-dover/\n",
      "2022-06-15 06:36:25.744 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/fresh-air-dover/ scraped successfully: Link number 96\n",
      "2022-06-15 06:36:26.691 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/bridge-to-life-group/\n",
      "2022-06-15 06:36:26.692 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/bridge-to-life-group/ scraped successfully: Link number 97\n",
      "2022-06-15 06:36:27.031 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/amagansett-saturday-night/\n",
      "2022-06-15 06:36:27.032 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/amagansett-saturday-night/ scraped successfully: Link number 98\n",
      "2022-06-15 06:36:27.362 | INFO     | __main__:fetch_soup_data:35 - got soup data for link: https://www.aa-meetings.com/aa-meeting/saturdays-in-the-park/\n",
      "2022-06-15 06:36:27.363 | INFO     | __main__:scrape:17 - [+] https://www.aa-meetings.com/aa-meeting/saturdays-in-the-park/ scraped successfully: Link number 99\n",
      "2022-06-15 06:36:32.371 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.385 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.388 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.391 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.398 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.412 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.415 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.416 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.419 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.425 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.427 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.428 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.431 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.437 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.438 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.439 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.440 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.443 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.444 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.445 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.446 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.451 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.452 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.453 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.456 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.462 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.463 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.463 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.466 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.471 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.474 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.475 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.477 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.482 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.483 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.484 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.485 | INFO     | __main__:get_address_data:28 - [+] Address data retrieved\n",
      "2022-06-15 06:36:32.488 | INFO     | __main__:get_table_data:95 - [+] Meeting Details Retrieved\n",
      "2022-06-15 06:36:32.489 | INFO     | __main__:soup_list_scraper:14 - [+] Address and table data parsed\n",
      "2022-06-15 06:36:32.490 | INFO     | __main__:parse_dicts:43 - [+] Row Data Parsed\n",
      "2022-06-15 06:36:32.491 | INFO     | __main__:soup_list_scraper:26 - ++++++++ SCRAPED SCRAPED BATCH OF LINKS +++++++++ \n",
      "2022-06-15 06:36:32.494 | INFO     | __main__:scrape:27 - [+] Count is divisible by 10:100 | Writing ROW_DATA to [csv_file] | soup_data and row_data cleared!\n",
      "2022-06-15 06:36:32.628 | INFO     | __main__:scrape:38 - <<<<<<<<<<<<<<< SUCCESS >>>>>>>>>>>>>>>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                 name                     address  \\\n",
      "0  [ Chillicothe First Capital Group]     [ 165 West 4th Street ]   \n",
      "1       [ Chillicothe Big Book Study]  [ 291 South Paint Street ]   \n",
      "2        [ Waverly Pike County Group]   [ 104 South High Street ]   \n",
      "3         [ One Step At A Time Group]       [ 211 Schmitt Drive ]   \n",
      "4                  [ Searsport Group]      [ 7 Knox Bros Avenue ]   \n",
      "5           [ East Millinocket Group]         [ 11 Maple Street ]   \n",
      "6         [ Togus VA Speaker Meeting]             [ 1 VA Center ]   \n",
      "7           [ Serenity Group Augusta]         [ 13 Green Street ]   \n",
      "8               [ Looney Toons Group]        [ 393 Water Street ]   \n",
      "9        [ 12th Step Being A Sponsor]    [ 6809 Guadalupe Street]   \n",
      "\n",
      "                 city    state zip_code                  day  \\\n",
      "0       [Chillicothe]   [Ohio]  [00000]             [Friday]   \n",
      "1       [Chillicothe]   [Ohio]  [00000]          [Wednesday]   \n",
      "2           [Waverly]   [Ohio]  [00000]          [Wednesday]   \n",
      "3           [Waverly]   [Ohio]  [00000]             [Monday]   \n",
      "4         [Searsport]  [Maine]  [00000]  [Saturday | Sunday]   \n",
      "5  [East Millinocket]  [Maine]  [00000]             [Sunday]   \n",
      "6           [Augusta]  [Maine]  [00000]   [Monday | Tuesday]   \n",
      "7           [Augusta]  [Maine]  [00000]           [Saturday]   \n",
      "8           [Augusta]  [Maine]  [00000]           [Saturday]   \n",
      "9            [Austin]  [Texas]  [00000]             [Sunday]   \n",
      "\n",
      "                                      time  \\\n",
      "0                      [7:30 pm - 8:30 pm]   \n",
      "1                      [7:00 pm - 8:00 pm]   \n",
      "2                      [7:30 pm - 8:30 pm]   \n",
      "3                      [7:00 pm - 8:00 pm]   \n",
      "4  [6:30 pm - 7:30 pm | 7:00 pm - 8:00 pm]   \n",
      "5                      [7:00 pm - 8:00 pm]   \n",
      "6  [7:00 pm - 8:00 pm | 7:00 pm - 8:00 pm]   \n",
      "7                      [7:00 pm - 8:00 pm]   \n",
      "8                    [10:00 am - 11:00 am]   \n",
      "9                    [10:00 am - 11:00 am]   \n",
      "\n",
      "                                                info  \n",
      "0  [Concurrent with Al-Anon, Open Meeting, Speake...  \n",
      "1  [Big Book, Discussion, Open Meeting, Wheelchai...  \n",
      "2      [Discussion, Open Meeting, Wheelchair Access]  \n",
      "3  [12 Steps & 12 Traditions, Open Meeting, Wheel...  \n",
      "4  [Open Meeting, Wheelchair Access | Discussion,...  \n",
      "5                            [Open Meeting, Speaker]  \n",
      "6  [Open Meeting, Speaker, Wheelchair Access | Op...  \n",
      "7      [Discussion, Open Meeting, Wheelchair Access]  \n",
      "8       [Discussion Open Meeting, Wheelchair Access]  \n",
      "9                         [Discussion, Open Meeting]  ,                                  name                     address  \\\n",
      "0                [ Way of Life Group]    [ 408 North East Street]   \n",
      "1       [ Grupo Libertad 10 de Junio]       [ 1920 Rusko Village]   \n",
      "2  [ Chillicothe First Capital Group]     [ 165 West 4th Street ]   \n",
      "3       [ Chillicothe Big Book Study]  [ 291 South Paint Street ]   \n",
      "4        [ Waverly Pike County Group]   [ 104 South High Street ]   \n",
      "5         [ One Step At A Time Group]       [ 211 Schmitt Drive ]   \n",
      "6                  [ Searsport Group]      [ 7 Knox Bros Avenue ]   \n",
      "7           [ East Millinocket Group]         [ 11 Maple Street ]   \n",
      "8         [ Togus VA Speaker Meeting]             [ 1 VA Center ]   \n",
      "9           [ Serenity Group Augusta]         [ 13 Green Street ]   \n",
      "\n",
      "                 city    state zip_code  \\\n",
      "0           [Atlanta]  [Texas]  [00000]   \n",
      "1            [Athens]  [Texas]  [00000]   \n",
      "2       [Chillicothe]   [Ohio]  [00000]   \n",
      "3       [Chillicothe]   [Ohio]  [00000]   \n",
      "4           [Waverly]   [Ohio]  [00000]   \n",
      "5           [Waverly]   [Ohio]  [00000]   \n",
      "6         [Searsport]  [Maine]  [00000]   \n",
      "7  [East Millinocket]  [Maine]  [00000]   \n",
      "8           [Augusta]  [Maine]  [00000]   \n",
      "9           [Augusta]  [Maine]  [00000]   \n",
      "\n",
      "                                                 day  \\\n",
      "0  [Tuesday | Thursday | Friday | Tuesday | Thurs...   \n",
      "1                                  [Monday | Friday]   \n",
      "2                                           [Friday]   \n",
      "3                                        [Wednesday]   \n",
      "4                                        [Wednesday]   \n",
      "5                                           [Monday]   \n",
      "6                                [Saturday | Sunday]   \n",
      "7                                           [Sunday]   \n",
      "8                                 [Monday | Tuesday]   \n",
      "9                                         [Saturday]   \n",
      "\n",
      "                                                time  \\\n",
      "0  [12:00 pm - 1:00 pm | 12:00 pm - 1:00 pm | 12:...   \n",
      "1            [7:00 pm - 9:00 pm | 7:00 pm - 9:00 pm]   \n",
      "2                                [7:30 pm - 8:30 pm]   \n",
      "3                                [7:00 pm - 8:00 pm]   \n",
      "4                                [7:30 pm - 8:30 pm]   \n",
      "5                                [7:00 pm - 8:00 pm]   \n",
      "6            [6:30 pm - 7:30 pm | 7:00 pm - 8:00 pm]   \n",
      "7                                [7:00 pm - 8:00 pm]   \n",
      "8            [7:00 pm - 8:00 pm | 7:00 pm - 8:00 pm]   \n",
      "9                                [7:00 pm - 8:00 pm]   \n",
      "\n",
      "                                                info  \n",
      "0  [Discussion, Open Meeting, Non-Smoking | Discu...  \n",
      "1  [Closed Meeting, Discussion, Spanish, Non-Smok...  \n",
      "2  [Concurrent with Al-Anon, Open Meeting, Speake...  \n",
      "3  [Big Book, Discussion, Open Meeting, Wheelchai...  \n",
      "4      [Discussion, Open Meeting, Wheelchair Access]  \n",
      "5  [12 Steps & 12 Traditions, Open Meeting, Wheel...  \n",
      "6  [Open Meeting, Wheelchair Access | Discussion,...  \n",
      "7                            [Open Meeting, Speaker]  \n",
      "8  [Open Meeting, Speaker, Wheelchair Access | Op...  \n",
      "9      [Discussion, Open Meeting, Wheelchair Access]  ,                                  name                      address  \\\n",
      "0               [ Looney Toons Group]         [ 393 Water Street ]   \n",
      "1        [ 12th Step Being A Sponsor]     [ 6809 Guadalupe Street]   \n",
      "2                [ Way of Life Group]     [ 408 North East Street]   \n",
      "3       [ Grupo Libertad 10 de Junio]        [ 1920 Rusko Village]   \n",
      "4                     [ Athens Group]     [ 408 West Tyler Street]   \n",
      "5  [ Contoocook Keep It Simple Group]              [ 24 Maple St ]   \n",
      "6            [ Triangle Group Online]       [ 401 Southwest Plaza]   \n",
      "7          [ AA Happy Thoughts Group]       [ 1330 Hooksett Road ]   \n",
      "8               [ Esperanza De Vivir]  [ 2323 West Lincoln Avenue]   \n",
      "9               [ Discussion Anaheim]         [ 202 West Broadway]   \n",
      "\n",
      "          city            state zip_code  \\\n",
      "0    [Augusta]          [Maine]  [00000]   \n",
      "1     [Austin]          [Texas]  [00000]   \n",
      "2    [Atlanta]          [Texas]  [00000]   \n",
      "3     [Athens]          [Texas]  [00000]   \n",
      "4     [Athens]          [Texas]  [00000]   \n",
      "5  [Hopkinton]  [New Hampshire]  [00000]   \n",
      "6  [Arlington]          [Texas]  [00000]   \n",
      "7   [Hooksett]  [New Hampshire]  [00000]   \n",
      "8    [Anaheim]     [California]  [00000]   \n",
      "9    [Anaheim]     [California]  [00000]   \n",
      "\n",
      "                                                 day  \\\n",
      "0                                         [Saturday]   \n",
      "1                                           [Sunday]   \n",
      "2  [Tuesday | Thursday | Friday | Tuesday | Thurs...   \n",
      "3                                  [Monday | Friday]   \n",
      "4  [Monday | Tuesday | Tuesday | Wednesday | Friday]   \n",
      "5                                           [Sunday]   \n",
      "6                                [Monday | Saturday]   \n",
      "7                                         [Thursday]   \n",
      "8  [Monday | Tuesday | Wednesday | Thursday | Fri...   \n",
      "9  [Wednesday | Wednesday | Thursday | Friday | S...   \n",
      "\n",
      "                                                time  \\\n",
      "0                              [10:00 am - 11:00 am]   \n",
      "1                              [10:00 am - 11:00 am]   \n",
      "2  [12:00 pm - 1:00 pm | 12:00 pm - 1:00 pm | 12:...   \n",
      "3            [7:00 pm - 9:00 pm | 7:00 pm - 9:00 pm]   \n",
      "4  [6:00 pm - 7:00 pm | 6:00 pm - 7:00 pm | 8:00 ...   \n",
      "5                                [7:00 pm - 8:00 pm]   \n",
      "6           [7:00 pm - 8:00 pm | 9:00 am - 10:00 am]   \n",
      "7                                [7:00 pm - 8:00 pm]   \n",
      "8  [8:00 pm - 9:00 pm | 8:00 pm - 9:00 pm | 8:00 ...   \n",
      "9  [12:00 pm - 1:00 pm | 5:30 pm - 6:30 pm | 5:30...   \n",
      "\n",
      "                                                info  \n",
      "0       [Discussion Open Meeting, Wheelchair Access]  \n",
      "1                         [Discussion, Open Meeting]  \n",
      "2  [Discussion, Open Meeting, Non-Smoking | Discu...  \n",
      "3  [Closed Meeting, Discussion, Spanish, Non-Smok...  \n",
      "4  [Discussion, Open Meeting, Wheelchair Access, ...  \n",
      "5  [Wheelchair Access, Speaker, Discussion, Concu...  \n",
      "6  [Big Book, Open Meeting | Big Book, Open Meeting]  \n",
      "7                              [Discussion, Virtual]  \n",
      "8  [Spanish | Spanish | Spanish | Spanish | Spani...  \n",
      "9  [Wheelchair Access, Open Meeting | Wheelchair ...  ,                                name                         address  \\\n",
      "0              [ In the Hall Group]      [ 50 Westminster Street  ]   \n",
      "1            [ Shining Light Group]            [ 162 Village Road ]   \n",
      "2              [ Carry The Message]   [ 727 South Harbor Boulevard]   \n",
      "3         [ Chester Big Book Group]            [ 1 Chester Street ]   \n",
      "4  [ Bills Babes Womens Book Study]        [ 311 West South Street]   \n",
      "5                 [ Big Book Study]            [ 202 West Broadway]   \n",
      "6            [ Saturday Night Live]      [ 2759 South King Street ]   \n",
      "7              [ Pali Womens Group]           [ 2747 Pali Highway ]   \n",
      "8                     [ Ohua Group]       [ 310 Paoakalani Avenue ]   \n",
      "9  [ New Beginnings Womens Meeting]  [ 5919 Kalanianaʻole Highway ]   \n",
      "\n",
      "         city            state zip_code  \\\n",
      "0   [Walpole]  [New Hampshire]  [00000]   \n",
      "1   [Newbury]  [New Hampshire]  [00000]   \n",
      "2   [Anaheim]     [California]  [00000]   \n",
      "3   [Chester]  [New Hampshire]  [00000]   \n",
      "4   [Anaheim]     [California]  [00000]   \n",
      "5   [Anaheim]     [California]  [00000]   \n",
      "6  [Honolulu]         [Hawaii]  [00000]   \n",
      "7  [Honolulu]         [Hawaii]  [00000]   \n",
      "8  [Honolulu]         [Hawaii]  [00000]   \n",
      "9  [Honolulu]         [Hawaii]  [00000]   \n",
      "\n",
      "                                                 day  \\\n",
      "0                      [Monday | Wednesday | Friday]   \n",
      "1  [Monday | Tuesday | Wednesday | Thursday | Fri...   \n",
      "2                                         [Saturday]   \n",
      "3                                        [Wednesday]   \n",
      "4                                         [Thursday]   \n",
      "5                                           [Monday]   \n",
      "6                                         [Saturday]   \n",
      "7                                         [Thursday]   \n",
      "8  [Tuesday | Wednesday | Thursday | Friday | Sat...   \n",
      "9                                          [Tuesday]   \n",
      "\n",
      "                                                time  \\\n",
      "0  [7:00 pm - 8:00 pm | 7:00 pm - 8:00 pm | 7:00 ...   \n",
      "1  [8:30 am - 9:30 am | 8:30 am - 9:30 am | 8:30 ...   \n",
      "2                                [6:00 am - 7:00 am]   \n",
      "3                                [8:00 pm - 9:30 pm]   \n",
      "4                                [6:30 pm - 7:30 pm]   \n",
      "5                                [8:30 pm - 9:30 pm]   \n",
      "6                                [8:00 pm - 9:00 pm]   \n",
      "7                                [5:30 pm - 6:30 pm]   \n",
      "8  [7:30 pm - 8:30 pm | 7:30 pm - 8:30 pm | 7:30 ...   \n",
      "9                                [7:00 pm - 8:00 pm]   \n",
      "\n",
      "                                                info  \n",
      "0             [Discussion | Discussion | Discussion]  \n",
      "1  [Wheelchair Access, Discussion | Wheelchair Ac...  \n",
      "2                                     [Open Meeting]  \n",
      "3                                         [Big Book]  \n",
      "4                            [Closed Meeting, Women]  \n",
      "5                  [Wheelchair Access, Open Meeting]  \n",
      "6  [Discussion, English, Open Meeting, Speaker, V...  \n",
      "7  [Discussion, English, Open Meeting, Women, Vir...  \n",
      "8  [English, LGBTQ, Newcomer, Open Meeting, Virtu...  \n",
      "9  [Big Book, Child-Friendly, English, Newcomer, ...  ,                               name                     address          city  \\\n",
      "0              [ Malia Discussion]   [ 766 North King Street ]    [Honolulu]   \n",
      "1              [ Huntsville Group]       [ 309 Church Avenue ]  [Huntsville]   \n",
      "2                [ Squad 26 Group]          [ 1415 6th Avenue]       [Anoka]   \n",
      "3                [ Squad 23 Anoka]  [ 2700 North Ferry Street]       [Anoka]   \n",
      "4                [ Squad 20 Anoka]  [ 2700 North Ferry Street]       [Anoka]   \n",
      "5          [ Saturday Night Fever]    [ 1631 Esmeralda Place ]      [Minden]   \n",
      "6                 [ Squad 2 Anoka]  [ 2700 North Ferry Street]       [Anoka]   \n",
      "7                [ Squad 18 Anoka]  [ 2700 North Ferry Street]       [Anoka]   \n",
      "8         [ Language of the Heart]     [ 850 West 4th Street ]      [Fallon]   \n",
      "9  [ Speaker and Birthday Meeting]    [ 457 Esmeralda Street ]      [Fallon]   \n",
      "\n",
      "         state zip_code                     day  \\\n",
      "0     [Hawaii]  [00000]              [Thursday]   \n",
      "1   [Arkansas]  [00000]  [Wednesday | Saturday]   \n",
      "2  [Minnesota]  [00000]              [Thursday]   \n",
      "3  [Minnesota]  [00000]                [Sunday]   \n",
      "4  [Minnesota]  [00000]              [Saturday]   \n",
      "5     [Nevada]  [00000]              [Saturday]   \n",
      "6  [Minnesota]  [00000]               [Tuesday]   \n",
      "7  [Minnesota]  [00000]             [Wednesday]   \n",
      "8     [Nevada]  [00000]             [Wednesday]   \n",
      "9     [Nevada]  [00000]              [Saturday]   \n",
      "\n",
      "                                      time  \\\n",
      "0                      [7:30 pm - 9:00 pm]   \n",
      "1  [8:00 pm - 9:00 pm | 8:00 pm - 9:00 pm]   \n",
      "2                      [7:30 pm - 8:30 pm]   \n",
      "3                      [7:30 pm - 8:30 pm]   \n",
      "4                      [8:00 am - 9:00 am]   \n",
      "5                      [7:00 pm - 8:00 pm]   \n",
      "6                      [8:00 pm - 9:00 pm]   \n",
      "7                      [6:30 pm - 7:30 pm]   \n",
      "8                      [5:15 pm - 6:15 pm]   \n",
      "9                      [7:00 pm - 8:00 pm]   \n",
      "\n",
      "                                                info  \n",
      "0  [Discussion, English, Open Meeting, Speaker, V...  \n",
      "1                    [Closed Meeting | Open Meeting]  \n",
      "2                                          [English]  \n",
      "3                  [Open Meeting, Wheelchair Access]  \n",
      "4           [Men, Closed Meeting, Wheelchair Access]  \n",
      "5      [Open Meeting, Discussion, Wheelchair Access]  \n",
      "6  [Child-Friendly, Open Meeting, Wheelchair Access]  \n",
      "7                  [Open Meeting, Wheelchair Access]  \n",
      "8      [Open Meeting, Discussion, Wheelchair Access]  \n",
      "9  [Birthday, Open Meeting, Speaker, Wheelchair A...  ,                                                 name  \\\n",
      "0                                  [ Wendover Group]   \n",
      "1                         [ Big Book Meeting Sparks]   \n",
      "2                  [ Eureka Group South Main Street]   \n",
      "3  [ Out Of The Fog, Out Of The Bog And Into The ...   \n",
      "4                                  [ Keep It Simple]   \n",
      "5                        [ Gratitude And Hope Group]   \n",
      "6                                  [ Franklin Group]   \n",
      "7                                  [ Fairview Group]   \n",
      "8                                    [ Way Out West]   \n",
      "9                         [ Tuesday Night Mens Stag]   \n",
      "\n",
      "                                address        city        state zip_code  \\\n",
      "0                  [ 915 Wells Avenue ]    [Nevada]      [89883]  [00000]   \n",
      "1    [ 500 Avenue De La Bleu De Clair ]    [Sparks]     [Nevada]  [00000]   \n",
      "2             [ 501 South Main Street ]    [Eureka]     [Nevada]  [00000]   \n",
      "3              [ 405 Murfreesboro Road]  [Franklin]  [Tennessee]  [00000]   \n",
      "4                [ 525 Sneed Road West]  [Franklin]  [Tennessee]  [00000]   \n",
      "5               [ 1216 Sneed Road West]  [Franklin]  [Tennessee]  [00000]   \n",
      "6                 [ 120 Aldersgate Way]  [Franklin]  [Tennessee]  [00000]   \n",
      "7                [ 7107 Westview Drive]  [Fairview]  [Tennessee]  [00000]   \n",
      "8  [ 26252 West Desert Vista Boulevard]   [Buckeye]    [Arizona]  [00000]   \n",
      "9        [ 20555 West Roosevelt Street]   [Buckeye]    [Arizona]  [00000]   \n",
      "\n",
      "                                                 day  \\\n",
      "0  [Tuesday | Wednesday | Thursday | Friday | Sat...   \n",
      "1                                           [Monday]   \n",
      "2                                           [Sunday]   \n",
      "3                                [Saturday | Sunday]   \n",
      "4                    [Monday | Wednesday | Saturday]   \n",
      "5                                           [Monday]   \n",
      "6  [Monday | Tuesday | Wednesday | Thursday | Fri...   \n",
      "7                               [Tuesday | Thursday]   \n",
      "8                                           [Monday]   \n",
      "9                                          [Tuesday]   \n",
      "\n",
      "                                                time  \\\n",
      "0  [7:00 pm - 8:00 pm | 7:00 pm - 8:00 pm | 7:00 ...   \n",
      "1                                [7:00 pm - 8:00 pm]   \n",
      "2                                [6:00 pm - 7:00 pm]   \n",
      "3            [6:30 am - 7:30 am | 6:30 am - 7:30 am]   \n",
      "4  [7:00 pm - 8:00 pm | 8:00 pm - 9:00 pm | 10:00...   \n",
      "5                                [7:00 pm - 8:00 pm]   \n",
      "6  [12:00 pm - 1:00 pm | 12:00 pm - 1:00 pm | 12:...   \n",
      "7            [7:30 pm - 8:30 pm | 7:30 pm - 8:30 pm]   \n",
      "8                                [7:00 pm - 8:00 pm]   \n",
      "9                                [7:00 pm - 8:00 pm]   \n",
      "\n",
      "                                                info  \n",
      "0  [Open Meeting | Open Meeting, Discussion | Ope...  \n",
      "1                           [Open Meeting, Big Book]  \n",
      "2      [Discussion, Open Meeting, Wheelchair Access]  \n",
      "3  [Discussion, Open Meeting | Discussion, Open M...  \n",
      "4  [Closed Meeting, Literature | Closed Meeting |...  \n",
      "5                         [Open Meeting, Discussion]  \n",
      "6  [Open Meeting, Discussion, Newcomer | Open Mee...  \n",
      "7  [Open Meeting, Discussion, Big Book | Open Mee...  \n",
      "8                         [Open Meeting, Discussion]  \n",
      "9                    [Open Meeting, Discussion, Men]  ,                                      name                         address  \\\n",
      "0               [ Surrender To Win Group]            [ 508 Monroe Avenue]   \n",
      "1                       [ Bone Dry Group]    [ 4239 North Village Street]   \n",
      "2           [ Black Canyon City AA Group]             [ 19001 Jacie Lane]   \n",
      "3                       [ Dry Dock Group]         [ 1901 Rolling Street ]   \n",
      "4                  [ Williams Hall Group]         [ 302 North Cody Road ]   \n",
      "5                       [ DeWitt Meeting]              [ 818 6th Avenue ]   \n",
      "6                  [ T G I S Group Alton]              [ 305 8th Street ]   \n",
      "7  [ Women in Recovery 12 and 12 Meeting]  [ 9100 Northeast 219th Street]   \n",
      "8                [ Saturday Morning BBSG]         [ 601 East Main Street]   \n",
      "9         [ By the Book BB 12 Step Study]  [ 9100 Northeast 219th Street]   \n",
      "\n",
      "                  city         state zip_code  \\\n",
      "0            [Buckeye]     [Arizona]  [00000]   \n",
      "1            [Buckeye]       [85396]  [00000]   \n",
      "2  [Black Canyon City]     [Arizona]  [00000]   \n",
      "3            [Ruthven]        [Iowa]  [00000]   \n",
      "4          [Le Claire]        [Iowa]  [00000]   \n",
      "5             [DeWitt]        [Iowa]  [00000]   \n",
      "6              [Alton]        [Iowa]  [00000]   \n",
      "7      [Battle Ground]  [Washington]  [00000]   \n",
      "8      [Battle Ground]  [Washington]  [00000]   \n",
      "9      [Battle Ground]  [Washington]  [00000]   \n",
      "\n",
      "                                      day  \\\n",
      "0                                [Sunday]   \n",
      "1                             [Wednesday]   \n",
      "2           [Meeting | 5:30 pm - 6:30 pm]   \n",
      "3                               [Tuesday]   \n",
      "4                                [Friday]   \n",
      "5  [Monday | Wednesday | Friday | Sunday]   \n",
      "6                              [Saturday]   \n",
      "7                              [Thursday]   \n",
      "8                              [Saturday]   \n",
      "9                               [Tuesday]   \n",
      "\n",
      "                                                time  \\\n",
      "0                                [7:00 pm - 8:00 pm]   \n",
      "1                                [7:00 pm - 8:00 pm]   \n",
      "2           [5:30 pm - 6:30 pm | 9:30 am - 10:30 am]   \n",
      "3                                [7:30 pm - 8:30 pm]   \n",
      "4                                [6:00 pm - 7:00 pm]   \n",
      "5  [12:00 pm - 1:00 pm | 12:00 pm - 1:00 pm | 12:...   \n",
      "6                               [9:00 am - 10:00 am]   \n",
      "7                               [12:00 pm - 1:00 pm]   \n",
      "8                                [8:00 am - 9:00 am]   \n",
      "9                                [5:30 pm - 6:30 pm]   \n",
      "\n",
      "                                                info  \n",
      "0                         [Open Meeting, Discussion]  \n",
      "1                  [Closed Meeting, Discussion, Men]  \n",
      "2  [Open Meeting | Open Meeting, 12 Steps & 12 Tr...  \n",
      "3                                   [Closed Meeting]  \n",
      "4                         [Discussion, Open Meeting]  \n",
      "5  [Big Book, Open Meeting | Big Book, Open Meeti...  \n",
      "6                                          [English]  \n",
      "7  [12 Steps & 12 Traditions, English, Open Meeti...  \n",
      "8  [Big Book, English, Open Meeting, Wheelchair A...  \n",
      "9  [Big Book, English, Literature, Open Meeting, ...  ,                                       name                          address  \\\n",
      "0              [ Battle Ground AA Meeting]  [ 11117 Northeast 189th Street]   \n",
      "1           [ Mens Spiritual Search Group]          [ 105 Winslow Way West]   \n",
      "2                  [ 12 and 12 Book Study]              [ 311 Depot Street]   \n",
      "3  [ St Annes Elementary School Saturdays]         [ 32 North Jones Street]   \n",
      "4   [ Emmanuel Lutheran Church Wednesdays]                   [ 960 U.S. 52]   \n",
      "5      [ Emmanuel Lutheran Church Mondays]                   [ 960 U.S. 52]   \n",
      "6               [ The Market Street Group]            [ 10 East 3rd Street]   \n",
      "7                 [ Mens Meeting Cheyenne]         [ 1904 East 15th Street]   \n",
      "8                   [ Group 1 at 300 Club]               [ 300 Derr Avenue]   \n",
      "9          [ Thursday Night Royal Meeting]            [ 207 Church Street ]   \n",
      "\n",
      "                  city         state zip_code  \\\n",
      "0      [Battle Ground]  [Washington]  [00000]   \n",
      "1  [Bainbridge Island]  [Washington]  [00000]   \n",
      "2            [Antioch]    [Illinois]  [00000]   \n",
      "3              [Amboy]    [Illinois]  [00000]   \n",
      "4              [Amboy]    [Illinois]  [00000]   \n",
      "5              [Amboy]    [Illinois]  [00000]   \n",
      "6              [Alton]    [Illinois]  [00000]   \n",
      "7           [Cheyenne]     [Wyoming]  [00000]   \n",
      "8           [Cheyenne]     [Wyoming]  [00000]   \n",
      "9             [Royal ]        [Iowa]  [00000]   \n",
      "\n",
      "                                                 day  \\\n",
      "0           [Monday | Wednesday | Thursday | Friday]   \n",
      "1                                        [Wednesday]   \n",
      "2                                        [Wednesday]   \n",
      "3                                         [Saturday]   \n",
      "4                                        [Wednesday]   \n",
      "5                                           [Monday]   \n",
      "6                                           [Sunday]   \n",
      "7                                        [Wednesday]   \n",
      "8  [Monday | Tuesday | Wednesday | Thursday | Thu...   \n",
      "9                                         [Thursday]   \n",
      "\n",
      "                                                time  \\\n",
      "0  [6:00 pm - 7:00 pm | 6:00 pm - 7:00 pm | 6:00 ...   \n",
      "1                                [7:30 pm - 8:30 pm]   \n",
      "2                                [6:10 pm - 7:10 pm]   \n",
      "3                               [9:30 am - 10:30 am]   \n",
      "4                                [7:00 pm - 8:00 pm]   \n",
      "5                                [7:00 pm - 8:00 pm]   \n",
      "6                                [4:00 pm - 5:00 pm]   \n",
      "7                                [6:30 pm - 7:30 pm]   \n",
      "8  [5:30 pm - 6:30 pm | 5:30 pm - 6:30 pm | 5:30 ...   \n",
      "9                                [8:00 pm - 9:00 pm]   \n",
      "\n",
      "                                                info  \n",
      "0  [English, Open Meeting | English, Open Meeting...  \n",
      "1           [Closed Meeting, Wheelchair Access, Men]  \n",
      "2                                   [Closed Meeting]  \n",
      "3                                   [Closed Meeting]  \n",
      "4                                     [Open Meeting]  \n",
      "5                                     [Open Meeting]  \n",
      "6             [Discussion, Literature, Open Meeting]  \n",
      "7                    [Discussion, Men, Open Meeting]  \n",
      "8                      [ |  |  |  |  |  |  |  |  | ]  \n",
      "9                                   [Closed Meeting]  ,                                      name                     address  \\\n",
      "0               [ Fellowship in Recovery]     [ 2310 East 8th Street]   \n",
      "1                        [ Sun Fun Group]   [ 706 14th Avenue South ]   \n",
      "2           [ Eye Openers Group Cheyenne]     [ 108 East 18th Street]   \n",
      "3                         [ Poplar Group]      [ 93 Oak Drive North ]   \n",
      "4  [ Eye Openers Group Beginners Meeting]     [ 108 East 18th Street]   \n",
      "5             [ North Myrtle Beach Group]  [ 1100 33rd Avenue South ]   \n",
      "6                   [ Love And Tolerance]   [ 801 11th Avenue North ]   \n",
      "7                        [ Choices Group]      [ 1674 Liberty Street]   \n",
      "8                         [ Oconee Group]     [ 505 Bountyland Road ]   \n",
      "9          [ Happy Joyous and Free Group]              [ 61 Carey St]   \n",
      "\n",
      "                   city             state zip_code  \\\n",
      "0            [Cheyenne]         [Wyoming]  [00000]   \n",
      "1  [North Myrtle Beach]  [South Carolina]  [00000]   \n",
      "2            [Cheyenne]         [Wyoming]  [00000]   \n",
      "3       [Myrtle Beach ]  [South Carolina]  [00000]   \n",
      "4            [Cheyenne]         [Wyoming]  [00000]   \n",
      "5  [North Myrtle Beach]  [South Carolina]  [00000]   \n",
      "6  [North Myrtle Beach]  [South Carolina]  [00000]   \n",
      "7            [Ashville]    [Pennsylvania]  [00000]   \n",
      "8         [Westminster]  [South Carolina]  [00000]   \n",
      "9              [Ashley]    [Pennsylvania]  [00000]   \n",
      "\n",
      "                                                 day  \\\n",
      "0                                [Monday | Thursday]   \n",
      "1  [Tuesday | Wednesday | Thursday | Friday | Fri...   \n",
      "2  [Monday | Tuesday | Wednesday | Thursday | Fri...   \n",
      "3                                           [Monday]   \n",
      "4                                           [Sunday]   \n",
      "5                              [Tuesday | Wednesday]   \n",
      "6                                         [Saturday]   \n",
      "7                                        [Wednesday]   \n",
      "8                    [Tuesday | Thursday | Saturday]   \n",
      "9                                        [Wednesday]   \n",
      "\n",
      "                                                time  \\\n",
      "0            [7:00 pm - 8:00 pm | 7:00 pm - 8:00 pm]   \n",
      "1  [12:00 pm - 1:00 pm | 12:00 pm - 1:00 pm | 12:...   \n",
      "2  [7:15 am - 8:15 am | 7:15 am - 8:15 am | 7:15 ...   \n",
      "3                                [1:00 pm - 2:00 pm]   \n",
      "4                                [7:00 pm - 8:00 pm]   \n",
      "5           [7:00 pm - 8:00 pm | 12:00 pm - 1:00 pm]   \n",
      "6                                [8:30 am - 9:30 am]   \n",
      "7                                [7:00 pm - 8:00 pm]   \n",
      "8  [8:00 pm - 9:00 pm | 8:00 pm - 9:00 pm | 8:00 ...   \n",
      "9                                [6:30 pm - 7:30 pm]   \n",
      "\n",
      "                                                info  \n",
      "0  [Discussion, Men, Open Meeting, Wheelchair Acc...  \n",
      "1  [Open Meeting, Discussion | Discussion, Open M...  \n",
      "2  [Discussion, Open Meeting, Wheelchair Access |...  \n",
      "3                         [Discussion, Open Meeting]  \n",
      "4        [Open Meeting, Wheelchair Access, Newcomer]  \n",
      "5  [Big Book, Closed Meeting | Closed Meeting, Li...  \n",
      "6                         [Discussion, Open Meeting]  \n",
      "7                                       [Discussion]  \n",
      "8  [Open Meeting, Discussion, Wheelchair Access |...  \n",
      "9                            [Women, Closed Meeting]  ,                            name                           address  \\\n",
      "0      [ Higher Learners Group]                [ 101 College Ave]   \n",
      "1         [ Fountain Roc Group]      [ 425 North Dupont Highway ]   \n",
      "2   [ New Year Group Bayhealth]        [ 640 South State Street ]   \n",
      "3        [ Turning Point Group]                [ 827 19th Street]   \n",
      "4       [ Going To Any Lengths]  [ 1818 North Little Creek Road ]   \n",
      "5        [ Toast Masters Group]                [ 2009 6th Avenue]   \n",
      "6            [ Fresh Air Dover]      [ 46 South Bradford Street ]   \n",
      "7       [ Bridge to Life Group]               [ 756 Main Street ]   \n",
      "8  [ Amagansett Saturday Night]                [ 350 Main Street]   \n",
      "9      [ Saturdays in the Park]               [ 514 Park Avenue ]   \n",
      "\n",
      "           city           state zip_code                   day  \\\n",
      "0    [Annville]  [Pennsylvania]  [00000]             [Tuesday]   \n",
      "1       [Dover]      [Delaware]  [00000]  [Tuesday | Thursday]   \n",
      "2       [Dover]      [Delaware]  [00000]              [Sunday]   \n",
      "3     [Altoona]  [Pennsylvania]  [00000]              [Sunday]   \n",
      "4       [Dover]      [Delaware]  [00000]            [Thursday]   \n",
      "5     [Altoona]  [Pennsylvania]  [00000]            [Saturday]   \n",
      "6       [Dover]      [Delaware]  [00000]    [Tuesday | Friday]   \n",
      "7       [Dover]      [Delaware]  [00000]  [Wednesday | Friday]   \n",
      "8  [Amagansett]      [New York]  [00000]            [Saturday]   \n",
      "9  [Portsmouth]  [Rhode Island]  [00000]            [Saturday]   \n",
      "\n",
      "                                      time  \\\n",
      "0                      [5:15 pm - 6:15 pm]   \n",
      "1  [8:00 pm - 9:00 pm | 8:00 pm - 9:00 pm]   \n",
      "2                     [9:30 am - 10:30 am]   \n",
      "3                      [8:00 pm - 9:00 pm]   \n",
      "4                     [9:00 pm - 10:00 pm]   \n",
      "5                      [8:00 pm - 9:00 pm]   \n",
      "6  [7:00 pm - 8:00 pm | 7:00 pm - 8:00 pm]   \n",
      "7  [1:00 pm - 2:00 pm | 1:00 pm - 2:00 pm]   \n",
      "8                      [6:30 pm - 7:30 pm]   \n",
      "9                    [10:00 am - 11:00 am]   \n",
      "\n",
      "                                                info  \n",
      "0                         [Discussion, Open Meeting]  \n",
      "1  [12 Steps & 12 Traditions, Open Meeting | 12 S...  \n",
      "2                                [Temporary Closure]  \n",
      "3                                       [Discussion]  \n",
      "4  [Child-Friendly, Discussion, English, Newcomer...  \n",
      "5                                       [Discussion]  \n",
      "6            [Temporary Closure | Temporary Closure]  \n",
      "7            [Temporary Closure | Temporary Closure]  \n",
      "8                       [Closed Meeting, Discussion]  \n",
      "9                         [Discussion, Open Meeting]  ]\n"
     ]
    }
   ],
   "source": [
    "scrape(links_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Old Functions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraped to 13180\n",
    "new_csv_file = './meeting_details.csv'\n",
    "headers = [\"name\", 'address', 'city', 'state',\n",
    "           'zip_code', 'day', 'time', 'info']\n",
    "\n",
    "\n",
    "def csv_writer(row_data, csv_filename, headers=None):\n",
    "    with open(csv_filename, 'w') as f:\n",
    "        if headers is not None:\n",
    "            writer = csv.DictWriter(f, fieldnames=headers, delimiter='|')\n",
    "            writer.writeheader(headers)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "        else:\n",
    "            fieldnames = [k for k in row_data.keys()]\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='|')\n",
    "            writer.writeheader(fieldnames)\n",
    "            for row in row_data:\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "csv_writer(row_data1, csv_filename, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c957a87b781b1e0e6e953571db6a2c4b79eab3b2dd8ca59b5a7c957ad56688c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('scrapevenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
