{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from multiprocessing.pool import Pool, ThreadPool\n",
    "from multiprocessing.pool import MaybeEncodingError\n",
    "import multiprocessing\n",
    "import itertools as it\n",
    "\n",
    "# this is where any files or directories/path variables go:\n",
    "csv_filename = './meetings.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILDING A BASIC SCRAPER:\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A CSV PARSER TO GET LINKS FROM CSV FILE:\n",
    "def csv_parser(csv_reader, header: str):\n",
    "    _header = next(csv_reader)\n",
    "    headerIndex = _header.index(header)\n",
    "    # now create an empty list to append the addresses to\n",
    "    data_list = []\n",
    "    # loop through CSV file and append to address_list\n",
    "    for line in csv_reader:\n",
    "        all_data = line[headerIndex]\n",
    "        data_list.append(all_data)\n",
    "    return data_list\n",
    "\n",
    "# CREATE A CSV WRITER TO WRITE ROW DATA TO A NEW CSV FILE WITH MEETING DETAILS:\n",
    "def csv_writer(csv_writer, data):\n",
    "    # Open CSV file to append data to:\n",
    "    with open('new_meeting_data.csv', 'w') as f:\n",
    "        csv_writer = csv.DictWriter(f, field_names=list(data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "links = get_links(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ADDRESS DATA FROM EACH LINK IN \n",
    "# LINK LIST:\n",
    "def get_address_data(soup):\n",
    "        try:\n",
    "            address_tag = soup.address\n",
    "            address = address_tag.contents[1]\n",
    "            \n",
    "            meeting_name = soup.find(\n",
    "                'div', class_='fui-card-body').find(class_='weight-300')\n",
    "            name = meeting_name.contents[1]\n",
    "            \n",
    "            city_tag = meeting_name.find_next('a')\n",
    "            city = city_tag.contents[0]\n",
    "            \n",
    "            state_tag = city_tag.find_next('a')\n",
    "            state = state_tag.contents[0]\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "            \n",
    "        except IndexError as ie:\n",
    "            print(f\"Index Error: {ie}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL EXTRACT ALL THE TABLE DATA FROM EACH LINK\n",
    "# IN THE LINK LIST. THE TABLE DATA WILL THEN NEED TO BE PARSED AND \n",
    "# CLEANED IF THERE ARE MULTIPLE ITEMS:\n",
    "def get_table_data(soup):\n",
    "        try:\n",
    "            info_table = soup.find('table', class_='table fui-table')\n",
    "            # obtain all the columns from <th>\n",
    "            headers = []\n",
    "            for i in info_table.find_all('th'):\n",
    "                title = i.text\n",
    "                headers.append(title.lower())\n",
    "\n",
    "                # now create a dataframe:\n",
    "            df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        \n",
    "            # Now create the foor loop to fill dataframe\n",
    "            # a row is under the <tr> tags and data under the <td> tags\n",
    "            for j in info_table.find_all('tr')[1:]:\n",
    "                # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "                #     print(\"No info table found\")\n",
    "                row_data = j.find_all('td')\n",
    "                row = [i.text for i in row_data]\n",
    "                length = len(df)\n",
    "                df.loc[length] = row\n",
    "\n",
    "            # data['day'].append(df['day'].to_list())\n",
    "            # data['time'].append(df['time'].to_list())\n",
    "            # data['info'].append(df['info'].to_list())\n",
    "            day = df['day'].to_list()\n",
    "            time = df['time'].to_list()\n",
    "            info = df['info'].to_list()\n",
    "\n",
    "            # now return data\n",
    "            return {'day': day, 'time': time, 'info': info}\n",
    "        \n",
    "        except AttributeError as ae:\n",
    "            print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "            return {'day': 'day', 'time': 'time', 'info': 'info'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A FUNCTION THAT WILL PARSE THE ROW DATA AND STORE IT \n",
    "# IN A DICTIONARY. THAT DICTIONARY CAN THEN BE INSERTED INTO \n",
    "# A LIST OF DICTIONARIES CALLED ROW_LIST BUT TO DO THIS THE PARSER\n",
    "# HAS TO JOIN LIST ITEMS INTO ONE LONG STRING SO EACH ROW HAS THE \n",
    "# SAME NUMBER OF COLUMNS:\n",
    "# THIS WAS INSERTED INTO meeting_data_scraper\n",
    "def meeting_row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "            # now add item1 to the row data\n",
    "            row['day'] = ' | '.join(item1['day'])\n",
    "            row['time'] = ' | '.join(item1['time'])\n",
    "            row['info'] = ' | '.join(item1['info'])\n",
    "            # now return the row data dictionary\n",
    "            return row\n",
    "        except Exception as e:\n",
    "            print(f'{e}')\n",
    "    except Exception as e:\n",
    "        print(f'{e}')\n",
    "        for k, v in row.items():\n",
    "            if v is not None:\n",
    "                pass\n",
    "            else:\n",
    "                v = str(k)\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE 'MAIN LOGICAL FUNCTION' THIS FUNCTION WILL COMBINE THE \n",
    "# get_address_data, get_table_data, and meeting_row_parser FUNCTIONS.\n",
    "# THAT WAY I CAN EXECUTE ALL OF THE FUNCTIONS IN ONE CALL.\n",
    "def meeting_data_scraper(link):\n",
    "    \n",
    "    def get_address_data(soup):\n",
    "        \n",
    "        try:\n",
    "            address_tag = soup.address\n",
    "            address = address_tag.contents[1]\n",
    "            \n",
    "            meeting_name = soup.find(\n",
    "                'div', class_='fui-card-body').find(class_='weight-300')\n",
    "            name = meeting_name.contents[1]\n",
    "            \n",
    "            city_tag = meeting_name.find_next('a')\n",
    "            city = city_tag.contents[0]\n",
    "            \n",
    "            state_tag = city_tag.find_next('a')\n",
    "            state = state_tag.contents[0]\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "            \n",
    "        except IndexError as ie:\n",
    "            print(f\"Index Error: {ie}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "             \n",
    "    def get_table_data(soup):\n",
    "        try:\n",
    "            info_table = soup.find('table', class_='table fui-table')\n",
    "            # obtain all the columns from <th>\n",
    "            headers = []\n",
    "            for i in info_table.find_all('th'):\n",
    "                title = i.text\n",
    "                headers.append(title.lower())\n",
    "\n",
    "                # now create a dataframe:\n",
    "            df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        \n",
    "            # Now create the foor loop to fill dataframe\n",
    "            # a row is under the <tr> tags and data under the <td> tags\n",
    "            for j in info_table.find_all('tr')[1:]:\n",
    "                # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "                #     print(\"No info table found\")\n",
    "                row_data = j.find_all('td')\n",
    "                row = [i.text for i in row_data]\n",
    "                length = len(df)\n",
    "                df.loc[length] = row\n",
    "\n",
    "            # data['day'].append(df['day'].to_list())\n",
    "            # data['time'].append(df['time'].to_list())\n",
    "            # data['info'].append(df['info'].to_list())\n",
    "            day = df['day'].to_list()\n",
    "            time = df['time'].to_list()\n",
    "            info = df['info'].to_list()\n",
    "\n",
    "            # now return data\n",
    "            return {'day': day, 'time': time, 'info': info}\n",
    "        \n",
    "        except AttributeError as ae:\n",
    "            print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "            return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "\n",
    "    def meeting_row_parser(item0, item1):\n",
    "        \"\"\"\n",
    "        :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "        the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "        :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "        access the data -> Keys: 'day' - 'time' - 'info'\n",
    "        \n",
    "        create a final dictionary that will be used to store the information in the database as one row. \n",
    "        I will need to join the list items to create one string with a | seperating each item so I can \n",
    "        split the string when retrieving the data.\n",
    "        \"\"\"\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            row['name'] = 'name'\n",
    "            row['address'] = 'address'\n",
    "            row['city'] = 'city'\n",
    "            row['state'] = 'state'\n",
    "\n",
    "        # now add item1 to the row data\n",
    "        row['day'] = ' | '.join(item1['day'])\n",
    "        row['time'] = ' | '.join(item1['time'])\n",
    "        row['info'] = ' | '.join(item1['info'])\n",
    "\n",
    "        # now return the row data dictionary\n",
    "        return row\n",
    "    \n",
    "    # STEP 1: Get the webpage response obj from requests\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # STEP 2: Get Soup object for html parsing\n",
    "    soup = bs(page.text, \"lxml\")\n",
    "\n",
    "    # Create two dicts with the following keys\n",
    "    address_dict = get_address_data(soup)\n",
    "    details_dict = get_table_data(soup)\n",
    "\n",
    "    d = [address_dict, details_dict]\n",
    "    row_data = meeting_row_parser(d[0], d[1])\n",
    "    \n",
    "    return row_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "    \"Flatten one level of nesting\"\n",
    "    return it.chain.from_iterable(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole list of links\n",
    "link_list = get_links(csv_filename)\n",
    "\n",
    "# shortened list: first 1000\n",
    "links_1000 = link_list[:1001]\n",
    "\n",
    "# shortened list: first 500\n",
    "links_500 = link_list[:501]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = './meetings.csv'\n",
    "link_list = get_links(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CREATE FUNCTION THAT WILL GET THE PAGE CONTENTS AND CREATE SOUP OBJ\n",
    "def fetch_soup_data(link):\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.text, 'lxml')\n",
    "    return soup\n",
    "\n",
    "# TODO: CREATE FUNCTION THAT WILL RETRIEVE THE ADDRESS DATA AND RETURN DICT\n",
    "def get_address_data(soup):\n",
    "\n",
    "    try:\n",
    "        address_tag = soup.address\n",
    "        address = address_tag.contents[1]\n",
    "\n",
    "        meeting_name = soup.find(\n",
    "            'div', class_='fui-card-body').find(class_='weight-300')\n",
    "        name = meeting_name.contents[1]\n",
    "\n",
    "        city_tag = meeting_name.find_next('a')\n",
    "        city = city_tag.contents[0]\n",
    "\n",
    "        state_tag = city_tag.find_next('a')\n",
    "        state = state_tag.contents[0]\n",
    "        return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "\n",
    "    except IndexError as ie:\n",
    "        print(f\"Index Error: {ie}\")\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "        try:\n",
    "            return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "        try:\n",
    "            return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "        try:\n",
    "            return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "        except UnboundLocalError as ule:\n",
    "            print(f\"UnboundLocalError: {ule}\")\n",
    "\n",
    "# TODO: CREATE FUNCTION THAT WILL RETRIEVE THE DETAILS DATA FROM AN HTML TABLE\n",
    "def get_table_data(soup):\n",
    "    try:\n",
    "        info_table = soup.find('table', class_='table fui-table')\n",
    "        # obtain all the columns from <th>\n",
    "        headers = []\n",
    "        for i in info_table.find_all('th'):\n",
    "            title = i.text\n",
    "            headers.append(title.lower())\n",
    "\n",
    "            # now create a dataframe:\n",
    "        df = pd.DataFrame(columns=headers)\n",
    "\n",
    "        # Now create the foor loop to fill dataframe\n",
    "        # a row is under the <tr> tags and data under the <td> tags\n",
    "        for j in info_table.find_all('tr')[1:]:\n",
    "            # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "            #     print(\"No info table found\")\n",
    "            row_data = j.find_all('td')\n",
    "            row = [i.text for i in row_data]\n",
    "            length = len(df)\n",
    "            df.loc[length] = row\n",
    "\n",
    "        # data['day'].append(df['day'].to_list())\n",
    "        # data['time'].append(df['time'].to_list())\n",
    "        # data['info'].append(df['info'].to_list())\n",
    "        day = df['day'].to_list()\n",
    "        time = df['time'].to_list()\n",
    "        info = df['info'].to_list()\n",
    "\n",
    "        # now return data\n",
    "        return {'day': day, 'time': time, 'info': info}\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "        return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "\n",
    "# TODO: PASS THOSE TWO DICTS TO A FUNCTION THAT WILL PARSE AND COMBINE THEM:\n",
    "def meeting_row_parser(item0, item1):\n",
    "    \"\"\"\n",
    "    :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "    the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "    :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "    access the data -> Keys: 'day' - 'time' - 'info'\n",
    "    \n",
    "    create a final dictionary that will be used to store the information in the database as one row. \n",
    "    I will need to join the list items to create one string with a | seperating each item so I can \n",
    "    split the string when retrieving the data.\n",
    "    \"\"\"\n",
    "    row = {}\n",
    "    try:\n",
    "        row['name'] = item0['name']\n",
    "        row['address'] = item0['address']\n",
    "        row['city'] = item0['city']\n",
    "        row['state'] = item0['state']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        row['name'] = 'name'\n",
    "        row['address'] = 'address'\n",
    "        row['city'] = 'city'\n",
    "        row['state'] = 'state'\n",
    "\n",
    "    # now add item1 to the row data\n",
    "    try:\n",
    "        row['day']='|'.join(item1['day'])\n",
    "        row['time']='|'.join(item1['time'])\n",
    "        row['info'] = '|'.join(item1['info'])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # now return the row data dictionary\n",
    "    return row\n",
    "\n",
    "\n",
    "# TODO: CREATE A FUNCTION THAT WILL WRITE EACH ITEM IN THE ROW_DATA LIST TO A CSV FILE\n",
    "# SAVING THE ROW VALUES IN THEIR KEY'S COLUMN:\n",
    "def convert_row_data_to_csv(row_data):\n",
    "    with open('meeting_details.csv', 'w') as csvfile:\n",
    "        for d in row_data:\n",
    "            csvwriter = csv.DictWriter(csvfile, delimiter=',', field_names=list(d.keys()))\n",
    "            csvwriter.writerow(d)\n",
    "\n",
    "# TODO: GET THE LINKED LIST FROM THE CSV FILE IN meetings.csv\n",
    "csv_filename = './meetings.csv'\n",
    "\n",
    "# create func to get list of links:\n",
    "def get_links(csv_filename):\n",
    "    with open(csv_filename, 'r') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        link_list = csv_parser(csv_reader, 'link')\n",
    "    return link_list\n",
    "\n",
    "# TODO: RETRIEVE THAT COMBINED DICT, AND APPEND TO AN EMPTY LIST CALLED ROW DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CREATE FUNCTION THAT WILL GET THE PAGE CONTENTS AND CREATE SOUP OBJ\n",
    "def fetch_soup_data(link):\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Get Links\n",
    "    csv_filename = './meetings.csv'\n",
    "    links = get_links(csv_filename)\n",
    "    # Now lets get the soup data from each link:\n",
    "    soup = [fetch_soup_data(link) for link in links[:100]]\n",
    "    # now create a list of dicts containing address and detail data\n",
    "    row_data = []\n",
    "    for each in soup:\n",
    "        address_data = get_address_data(each)\n",
    "        detail_data = get_table_data(each)\n",
    "        d = [address_data, detail_data]\n",
    "        row_data.append(meeting_row_parser(d[0], d[1]))\n",
    "    convert_row_data_to_csv(row_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meeting:\n",
    "    COUNT = 1\n",
    "    LINKS = []\n",
    "    \n",
    "    \n",
    "    def __init__(self, link):\n",
    "        self.new_csv_path = '~/projects/github/web-scraper/scraped_meeting_data/'\n",
    "        self.rowId = COUNT\n",
    "        self.soup = self._fetch_soup_data(link)\n",
    "        self.address = self._address_data(self.soup)\n",
    "        self.info = self._table_data(self.soup)\n",
    "        self.row = self._row_parser(self.address, self.info)\n",
    "        COUNT += 1\n",
    "        \n",
    "    def _fetch_soup_data(self, link):\n",
    "        page = requests.get(link)\n",
    "        soup = bs(page.text, 'lxml')\n",
    "        return soup\n",
    "    \n",
    "    def _address_data(self, soup):\n",
    "        try:\n",
    "            address_tag = soup.address\n",
    "            address = address_tag.contents[1]\n",
    "\n",
    "            meeting_name = soup.find(\n",
    "                'div', class_='fui-card-body').find(class_='weight-300')\n",
    "            name = meeting_name.contents[1]\n",
    "\n",
    "            city_tag = meeting_name.find_next('a')\n",
    "            city = city_tag.contents[0]\n",
    "\n",
    "            state_tag = city_tag.find_next('a')\n",
    "            state = state_tag.contents[0]\n",
    "            return {'name': name, 'address': address, 'city': city, 'state': state}\n",
    "\n",
    "        except IndexError as ie:\n",
    "            print(f\"Index Error: {ie}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': city, 'state': 'state'}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': address, 'city': 'city', 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': name, 'address': 'address', 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "            try:\n",
    "                return {'name': 'name', 'address': address, 'city': city, 'state': state}\n",
    "            except UnboundLocalError as ule:\n",
    "                print(f\"UnboundLocalError: {ule}\")\n",
    "                \n",
    "    def _table_data(self, soup):\n",
    "        \"\"\"\n",
    "        This extracts the table data from each link in the link_list object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            info_table = soup.find('table', class_='table fui-table')\n",
    "            # obtain all the columns from <th>\n",
    "            headers = []\n",
    "            for i in info_table.find_all('th'):\n",
    "                title = i.text\n",
    "                headers.append(title.lower())\n",
    "\n",
    "                # now create a dataframe:\n",
    "            df = pd.DataFrame(columns=headers)\n",
    "\n",
    "            # Now create the foor loop to fill dataframe\n",
    "            # a row is under the <tr> tags and data under the <td> tags\n",
    "            for j in info_table.find_all('tr')[1:]:\n",
    "                # if info_table.find_all('tr')[1:] == AttributeError.NoneType:\n",
    "                #     print(\"No info table found\")\n",
    "                row_data = j.find_all('td')\n",
    "                row = [i.text for i in row_data]\n",
    "                length = len(df)\n",
    "                df.loc[length] = row\n",
    "\n",
    "            # data['day'].append(df['day'].to_list())\n",
    "            # data['time'].append(df['time'].to_list())\n",
    "            # data['info'].append(df['info'].to_list())\n",
    "            day = df['day'].to_list()\n",
    "            time = df['time'].to_list()\n",
    "            info = df['info'].to_list()\n",
    "\n",
    "            # now return data\n",
    "            return {'day': day, 'time': time, 'info': info}\n",
    "\n",
    "        except AttributeError as ae:\n",
    "            print(f\"info_table.find_all('tr')[1:] raised error: {ae}\")\n",
    "            return {'day': 'day', 'time': 'time', 'info': 'info'}\n",
    "\n",
    "    def _row_parser(self, item0, item1):\n",
    "        \"\"\"\n",
    "        :param item0: This is the address data in a dictionary. Use the following keys to access\n",
    "        the data -> Keys: 'name' - 'address' - 'city' - 'state' \n",
    "        :param item1: This is the meeting details data in a dictionary. Use the following keys to\n",
    "        access the data -> Keys: 'day' - 'time' - 'info'\n",
    "        \n",
    "        create a final dictionary that will be used to store the information in the database as one row. \n",
    "        I will need to join the list items to create one string with a | seperating each item so I can \n",
    "        split the string when retrieving the data.\n",
    "        \"\"\"\n",
    "        row = {}\n",
    "        try:\n",
    "            row['name'] = item0['name']\n",
    "            row['address'] = item0['address']\n",
    "            row['city'] = item0['city']\n",
    "            row['state'] = item0['state']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            row['name'] = 'name'\n",
    "            row['address'] = 'address'\n",
    "            row['city'] = 'city'\n",
    "            row['state'] = 'state'\n",
    "\n",
    "        # now add item1 to the row data\n",
    "        try:\n",
    "            row['day']='|'.join(item1['day'])\n",
    "            row['time']='|'.join(item1['time'])\n",
    "            row['info'] = '|'.join(item1['info'])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # now return the row data dictionary\n",
    "        return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeetingScraperInterface(Meeting):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Meeting, self).__init__()\n",
    "        rowId = Meeting.COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': 0    Saturday\n",
      "Name: day, dtype: object, 'time': 0    10:00 am - 11:00 am\n",
      "Name: time, dtype: object, 'info': 0    Discussion, Open Meeting\n",
      "Name: info, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "# list comprehension for address_data and detail_data\n",
    "for each in soup_data:\n",
    "    address_data = get_address_data(each)\n",
    "    detail_data = get_table_data(each)\n",
    "    d = [address_data, detail_data]\n",
    "print(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_row_data_to_list(data: dict):\n",
    "    # empty list to append individual data to\n",
    "    dict_list = []\n",
    "\n",
    "    # Now loop through the CSV file and append to the data list:\n",
    "    for k, v in data.items():\n",
    "        row_data = {k: v}\n",
    "        dict_list.append(row_data)\n",
    "    return dict_list\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c957a87b781b1e0e6e953571db6a2c4b79eab3b2dd8ca59b5a7c957ad56688c2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('scrapevenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
